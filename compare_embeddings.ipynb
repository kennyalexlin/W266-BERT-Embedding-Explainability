{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request, json, os, math\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import (\n",
        "    TFBertForMaskedLM,\n",
        "    PreTrainedTokenizerFast,\n",
        ")\n",
        "\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "nHT_t2r-M5qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    import sys\n",
        "\n",
        "    drive.mount('/content/gdrive/')\n",
        "    sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from utils import (\n",
        "    get_token_embedding,\n",
        "    get_k_nearest_neighbors,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5vzgkxGNLtD",
        "outputId": "cb23a203-8245-4954-ad5c-2a605b056d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_DIR = '/content/gdrive/My Drive/Colab Notebooks/w266_final_proj'\n",
        "\n",
        "TOKENIZER1_PATH = 'birthyear.1990_2009.lowercase_tokenizer'\n",
        "TOKENIZER2_PATH = 'birthyear.1950_1969.lowercase_tokenizer'\n",
        "\n",
        "MODEL1_PATH = f'birthyear.1990_2009.lowercase_64batch_size_20000steps'\n",
        "MODEL2_PATH = f'birthyear.1950_1969.lowercase_64batch_size_20000steps'\n",
        "\n",
        "# path to load trained tokenizers from\n",
        "full_tokenizer1_path = os.path.join(PROJECT_DIR, TOKENIZER1_PATH)\n",
        "full_tokenizer2_path = os.path.join(PROJECT_DIR, TOKENIZER2_PATH)\n",
        "\n",
        "# path to load trained BERT model2 from\n",
        "full_model1_path = os.path.join(PROJECT_DIR, MODEL1_PATH)\n",
        "full_model2_path = os.path.join(PROJECT_DIR, MODEL2_PATH)"
      ],
      "metadata": {
        "id": "r-Lo8NEAM0NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models"
      ],
      "metadata": {
        "id": "nI8cf2DpQHFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = PreTrainedTokenizerFast.from_pretrained(full_tokenizer1_path)\n",
        "tokenizer2 = PreTrainedTokenizerFast.from_pretrained(full_tokenizer2_path)\n",
        "\n",
        "bert_model1 = TFBertForMaskedLM.from_pretrained(full_model1_path)\n",
        "bert_model2 = TFBertForMaskedLM.from_pretrained(full_model2_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od2-Vtu8N6zY",
        "outputId": "a3b0c983-a922-4f8b-ff9c-5a8acceaa0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1990_2009.lowercase_64batch_size_20000steps were not used when initializing TFBertForMaskedLM: ['encoder/layer_._2/intermediate/dense/bias:0', 'encoder/layer_._3/attention/output/dense/bias:0', 'encoder/layer_._5/attention/self/query/kernel:0', 'encoder/layer_._2/output/LayerNorm/gamma:0', 'encoder/layer_._6/intermediate/dense/bias:0', 'encoder/layer_._10/attention/self/value/bias:0', 'predictions/transform/LayerNorm/beta:0', 'embeddings/token_type_embeddings/embeddings:0', 'encoder/layer_._7/output/dense/kernel:0', 'encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'encoder/layer_._2/attention/self/key/bias:0', 'encoder/layer_._5/output/LayerNorm/beta:0', 'encoder/layer_._9/intermediate/dense/kernel:0', 'encoder/layer_._8/attention/self/query/bias:0', 'encoder/layer_._3/output/LayerNorm/beta:0', 'encoder/layer_._5/intermediate/dense/bias:0', 'encoder/layer_._9/attention/self/query/bias:0', 'encoder/layer_._0/attention/self/value/kernel:0', 'encoder/layer_._7/output/LayerNorm/gamma:0', 'encoder/layer_._2/output/dense/bias:0', 'encoder/layer_._4/attention/output/dense/bias:0', 'predictions/transform/dense/bias:0', 'encoder/layer_._0/attention/output/LayerNorm/beta:0', 'encoder/layer_._10/attention/output/dense/bias:0', 'encoder/layer_._10/intermediate/dense/kernel:0', 'encoder/layer_._11/output/dense/kernel:0', 'encoder/layer_._2/output/LayerNorm/beta:0', 'encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/attention/self/key/kernel:0', 'encoder/layer_._1/attention/self/query/bias:0', 'encoder/layer_._2/attention/self/query/kernel:0', 'encoder/layer_._3/attention/self/query/bias:0', 'encoder/layer_._10/intermediate/dense/bias:0', 'encoder/layer_._0/intermediate/dense/bias:0', 'encoder/layer_._2/output/dense/kernel:0', 'encoder/layer_._5/attention/self/value/bias:0', 'encoder/layer_._5/output/dense/bias:0', 'encoder/layer_._2/attention/output/LayerNorm/beta:0', 'encoder/layer_._5/output/LayerNorm/gamma:0', 'encoder/layer_._2/attention/self/key/kernel:0', 'encoder/layer_._8/output/dense/bias:0', 'encoder/layer_._8/intermediate/dense/bias:0', 'encoder/layer_._2/attention/output/dense/kernel:0', 'encoder/layer_._9/attention/self/value/kernel:0', 'encoder/layer_._5/attention/self/query/bias:0', 'encoder/layer_._4/intermediate/dense/bias:0', 'predictions/transform/dense/kernel:0', 'encoder/layer_._1/output/dense/kernel:0', 'encoder/layer_._11/intermediate/dense/kernel:0', 'encoder/layer_._0/attention/self/query/kernel:0', 'encoder/layer_._7/attention/self/value/bias:0', 'encoder/layer_._9/intermediate/dense/bias:0', 'embeddings/LayerNorm/beta:0', 'encoder/layer_._3/intermediate/dense/bias:0', 'encoder/layer_._5/attention/self/key/bias:0', 'encoder/layer_._9/attention/output/dense/bias:0', 'encoder/layer_._6/attention/self/query/kernel:0', 'encoder/layer_._10/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/output/LayerNorm/beta:0', 'encoder/layer_._11/output/LayerNorm/beta:0', 'encoder/layer_._10/attention/self/query/kernel:0', 'encoder/layer_._2/attention/output/dense/bias:0', 'encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'encoder/layer_._8/attention/output/dense/kernel:0', 'encoder/layer_._8/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/self/key/bias:0', 'encoder/layer_._7/output/dense/bias:0', 'encoder/layer_._1/attention/output/LayerNorm/beta:0', 'encoder/layer_._8/attention/self/key/kernel:0', 'encoder/layer_._11/attention/output/dense/bias:0', 'encoder/layer_._11/output/LayerNorm/gamma:0', 'embeddings/LayerNorm/gamma:0', 'encoder/layer_._7/attention/self/query/bias:0', 'encoder/layer_._4/attention/output/LayerNorm/beta:0', 'embeddings/position_embeddings/embeddings:0', 'encoder/layer_._4/attention/self/key/bias:0', 'encoder/layer_._1/attention/self/value/bias:0', 'encoder/layer_._10/output/dense/bias:0', 'encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'encoder/layer_._5/attention/output/dense/kernel:0', 'encoder/layer_._9/attention/self/query/kernel:0', 'encoder/layer_._1/attention/self/value/kernel:0', 'encoder/layer_._1/attention/self/key/kernel:0', 'encoder/layer_._4/attention/output/dense/kernel:0', 'encoder/layer_._7/attention/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/output/dense/kernel:0', 'encoder/layer_._0/attention/output/dense/bias:0', 'encoder/layer_._3/attention/self/value/bias:0', 'encoder/layer_._3/attention/output/dense/kernel:0', 'encoder/layer_._5/output/dense/kernel:0', 'encoder/layer_._2/attention/self/value/bias:0', 'encoder/layer_._6/output/dense/kernel:0', 'encoder/layer_._10/attention/self/key/bias:0', 'encoder/layer_._0/output/LayerNorm/gamma:0', 'encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'encoder/layer_._3/output/dense/kernel:0', 'encoder/layer_._4/output/LayerNorm/beta:0', 'encoder/layer_._5/attention/self/value/kernel:0', 'encoder/layer_._1/intermediate/dense/kernel:0', 'encoder/layer_._11/attention/self/key/bias:0', 'encoder/layer_._2/intermediate/dense/kernel:0', 'embeddings/word_embeddings/weight:0', 'encoder/layer_._4/attention/self/key/kernel:0', 'encoder/layer_._3/attention/self/query/kernel:0', 'encoder/layer_._9/attention/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/self/value/bias:0', 'encoder/layer_._3/output/dense/bias:0', 'encoder/layer_._7/attention/output/dense/bias:0', 'encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'encoder/layer_._1/attention/self/key/bias:0', 'encoder/layer_._6/output/dense/bias:0', 'encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'encoder/layer_._7/intermediate/dense/bias:0', 'encoder/layer_._5/attention/output/dense/bias:0', 'encoder/layer_._1/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/self/query/kernel:0', 'encoder/layer_._8/attention/self/value/bias:0', 'encoder/layer_._6/attention/self/key/bias:0', 'encoder/layer_._1/attention/self/query/kernel:0', 'encoder/layer_._10/output/LayerNorm/gamma:0', 'encoder/layer_._4/attention/self/query/bias:0', 'encoder/layer_._0/intermediate/dense/kernel:0', 'encoder/layer_._8/attention/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/self/key/kernel:0', 'encoder/layer_._4/output/dense/kernel:0', 'encoder/layer_._0/attention/self/key/kernel:0', 'encoder/layer_._6/attention/self/query/bias:0', 'encoder/layer_._1/attention/output/dense/bias:0', 'encoder/layer_._0/output/dense/bias:0', 'encoder/layer_._6/attention/output/dense/bias:0', 'encoder/layer_._6/output/LayerNorm/gamma:0', 'encoder/layer_._5/attention/output/LayerNorm/beta:0', 'encoder/layer_._0/output/dense/kernel:0', 'predictions/bias:0', 'encoder/layer_._10/attention/output/dense/kernel:0', 'encoder/layer_._7/attention/self/key/kernel:0', 'encoder/layer_._7/attention/self/value/kernel:0', 'predictions/transform/LayerNorm/gamma:0', 'encoder/layer_._4/output/LayerNorm/gamma:0', 'encoder/layer_._0/attention/self/value/bias:0', 'encoder/layer_._1/output/dense/bias:0', 'encoder/layer_._8/attention/self/query/kernel:0', 'encoder/layer_._5/attention/self/key/kernel:0', 'encoder/layer_._1/output/LayerNorm/gamma:0', 'encoder/layer_._4/attention/self/value/kernel:0', 'encoder/layer_._3/attention/self/key/bias:0', 'encoder/layer_._8/attention/self/value/kernel:0', 'encoder/layer_._6/attention/self/value/bias:0', 'encoder/layer_._10/attention/self/key/kernel:0', 'encoder/layer_._7/attention/self/query/kernel:0', 'encoder/layer_._9/output/dense/bias:0', 'encoder/layer_._11/attention/self/query/bias:0', 'encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'encoder/layer_._9/attention/self/key/kernel:0', 'encoder/layer_._7/intermediate/dense/kernel:0', 'encoder/layer_._8/attention/self/key/bias:0', 'encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/attention/self/value/kernel:0', 'encoder/layer_._11/attention/self/value/kernel:0', 'encoder/layer_._4/output/dense/bias:0', 'encoder/layer_._6/attention/output/LayerNorm/beta:0', 'encoder/layer_._5/intermediate/dense/kernel:0', 'encoder/layer_._0/attention/self/query/bias:0', 'encoder/layer_._6/output/LayerNorm/beta:0', 'encoder/layer_._7/attention/output/dense/kernel:0', 'encoder/layer_._1/attention/output/dense/kernel:0', 'encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'encoder/layer_._11/attention/self/query/kernel:0', 'encoder/layer_._0/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/self/value/bias:0', 'encoder/layer_._11/intermediate/dense/bias:0', 'encoder/layer_._0/attention/self/key/bias:0', 'encoder/layer_._2/attention/self/query/bias:0', 'encoder/layer_._9/output/dense/kernel:0', 'encoder/layer_._8/output/LayerNorm/gamma:0', 'encoder/layer_._10/output/dense/kernel:0', 'encoder/layer_._2/attention/self/value/kernel:0', 'encoder/layer_._3/intermediate/dense/kernel:0', 'encoder/layer_._7/output/LayerNorm/beta:0', 'encoder/layer_._0/output/LayerNorm/beta:0', 'encoder/layer_._8/attention/output/dense/bias:0', 'encoder/layer_._11/output/dense/bias:0', 'encoder/layer_._8/output/dense/kernel:0', 'encoder/layer_._11/attention/self/value/bias:0', 'encoder/layer_._9/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/self/query/bias:0', 'encoder/layer_._3/attention/self/value/kernel:0', 'encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/output/LayerNorm/beta:0', 'encoder/layer_._8/intermediate/dense/kernel:0', 'encoder/layer_._7/attention/self/key/bias:0', 'encoder/layer_._3/attention/self/key/kernel:0', 'encoder/layer_._3/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/self/value/kernel:0', 'encoder/layer_._1/intermediate/dense/bias:0', 'encoder/layer_._4/intermediate/dense/kernel:0', 'encoder/layer_._9/output/LayerNorm/beta:0', 'encoder/layer_._3/attention/output/LayerNorm/beta:0', 'encoder/layer_._6/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/intermediate/dense/kernel:0']\n",
            "- This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForMaskedLM were not initialized from the model checkpoint at /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1990_2009.lowercase_64batch_size_20000steps and are newly initialized: ['bert/encoder/layer_._10/attention/output/dense/bias:0', 'bert/encoder/layer_._0/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/output/dense/kernel:0', 'bert/encoder/layer_._10/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/self/query/bias:0', 'bert/encoder/layer_._8/attention/self/key/kernel:0', 'bert/encoder/layer_._11/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/self/query/bias:0', 'bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/intermediate/dense/kernel:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/self/query/bias:0', 'bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/output/dense/bias:0', 'bert/encoder/layer_._0/attention/self/key/bias:0', 'bert/encoder/layer_._1/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/attention/self/key/kernel:0', 'bert/encoder/layer_._4/attention/output/dense/bias:0', 'bert/encoder/layer_._3/output/dense/bias:0', 'bert/encoder/layer_._9/output/dense/bias:0', 'bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/intermediate/dense/kernel:0', 'bert/encoder/layer_._0/attention/self/key/kernel:0', 'bert/encoder/layer_._5/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/output/dense/bias:0', 'bert/encoder/layer_._7/attention/self/query/bias:0', 'bert/encoder/layer_._10/attention/self/value/bias:0', 'bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/query/kernel:0', 'bert/encoder/layer_._8/attention/output/dense/bias:0', 'bert/encoder/layer_._9/attention/self/key/kernel:0', 'bert/encoder/layer_._3/attention/self/value/kernel:0', 'bert/encoder/layer_._6/intermediate/dense/kernel:0', 'bert/encoder/layer_._9/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/intermediate/dense/kernel:0', 'bert/encoder/layer_._2/intermediate/dense/kernel:0', 'bert/encoder/layer_._1/attention/self/key/bias:0', 'bert/encoder/layer_._3/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/key/bias:0', 'bert/encoder/layer_._3/attention/output/dense/bias:0', 'bert/encoder/layer_._7/attention/output/dense/bias:0', 'bert/encoder/layer_._11/attention/self/key/bias:0', 'bert/encoder/layer_._5/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/output/dense/kernel:0', 'bert/encoder/layer_._5/output/dense/kernel:0', 'bert/encoder/layer_._10/attention/self/query/kernel:0', 'bert/encoder/layer_._5/attention/output/dense/bias:0', 'bert/encoder/layer_._5/attention/self/value/kernel:0', 'bert/encoder/layer_._10/intermediate/dense/kernel:0', 'bert/encoder/layer_._4/attention/self/key/bias:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/key/bias:0', 'bert/encoder/layer_._4/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/intermediate/dense/kernel:0', 'bert/encoder/layer_._1/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/self/key/kernel:0', 'bert/encoder/layer_._7/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/attention/self/value/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/kernel:0', 'bert/encoder/layer_._3/attention/self/key/bias:0', 'bert/encoder/layer_._1/attention/self/key/kernel:0', 'bert/encoder/layer_._4/attention/self/key/kernel:0', 'bert/encoder/layer_._4/output/dense/kernel:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/attention/self/query/kernel:0', 'bert/embeddings/position_embeddings/weight:0', 'bert/encoder/layer_._2/attention/self/value/kernel:0', 'bert/encoder/layer_._6/intermediate/dense/bias:0', 'bert/encoder/layer_._8/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/self/value/bias:0', 'bert/encoder/layer_._1/intermediate/dense/kernel:0', 'bert/encoder/layer_._10/attention/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/output/dense/bias:0', 'bert/encoder/layer_._3/attention/self/query/kernel:0', 'bert/encoder/layer_._7/attention/self/value/kernel:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/self/query/kernel:0', 'bert/encoder/layer_._0/attention/self/query/kernel:0', 'bert/encoder/layer_._10/output/dense/bias:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/output/dense/bias:0', 'bert/encoder/layer_._1/attention/self/value/kernel:0', 'bert/encoder/layer_._8/output/dense/bias:0', 'bert/encoder/layer_._5/intermediate/dense/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/bias:0', 'bert/encoder/layer_._2/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/self/query/kernel:0', 'bert/embeddings/token_type_embeddings/weight:0', 'bert/encoder/layer_._6/attention/self/key/kernel:0', 'bert/encoder/layer_._3/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/attention/self/value/bias:0', 'bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'bert/embeddings/LayerNorm/beta:0', 'bert/encoder/layer_._4/attention/self/value/bias:0', 'bert/encoder/layer_._10/attention/self/key/bias:0', 'bert/encoder/layer_._0/attention/output/dense/bias:0', 'bert/encoder/layer_._11/output/dense/bias:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/self/key/kernel:0', 'bert/encoder/layer_._11/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/query/kernel:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/self/key/bias:0', 'bert/encoder/layer_._7/output/dense/kernel:0', 'bert/encoder/layer_._10/intermediate/dense/bias:0', 'bert/encoder/layer_._1/output/dense/kernel:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/attention/self/key/bias:0', 'bert/encoder/layer_._7/attention/self/key/kernel:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/value/kernel:0', 'bert/encoder/layer_._2/attention/self/query/kernel:0', 'bert/encoder/layer_._6/attention/self/query/bias:0', 'bert/encoder/layer_._11/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/output/dense/kernel:0', 'bert/encoder/layer_._7/intermediate/dense/bias:0', 'bert/encoder/layer_._10/attention/self/key/kernel:0', 'bert/encoder/layer_._9/attention/output/dense/bias:0', 'mlm___cls/predictions/transform/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/query/bias:0', 'bert/encoder/layer_._0/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/value/bias:0', 'bert/encoder/layer_._10/attention/self/query/bias:0', 'bert/encoder/layer_._10/attention/self/value/kernel:0', 'bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/intermediate/dense/bias:0', 'bert/encoder/layer_._8/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/output/dense/bias:0', 'bert/encoder/layer_._4/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/query/kernel:0', 'bert/encoder/layer_._3/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/self/query/bias:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'mlm___cls/predictions/bias:0', 'bert/encoder/layer_._8/output/dense/kernel:0', 'bert/encoder/layer_._9/intermediate/dense/bias:0', 'mlm___cls/predictions/transform/dense/bias:0', 'bert/encoder/layer_._2/attention/self/value/bias:0', 'bert/encoder/layer_._8/intermediate/dense/bias:0', 'bert/encoder/layer_._3/attention/self/key/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/self/query/kernel:0', 'bert/encoder/layer_._0/output/dense/bias:0', 'bert/encoder/layer_._8/attention/self/key/bias:0', 'bert/encoder/layer_._6/attention/output/dense/bias:0', 'bert/encoder/layer_._8/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/output/dense/bias:0', 'bert/encoder/layer_._6/attention/self/query/kernel:0', 'bert/encoder/layer_._5/attention/self/query/bias:0', 'bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'bert/encoder/layer_._3/intermediate/dense/bias:0', 'bert/encoder/layer_._2/output/dense/bias:0', 'bert/embeddings/LayerNorm/gamma:0', 'bert/encoder/layer_._5/intermediate/dense/bias:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._8/attention/self/value/kernel:0', 'bert/encoder/layer_._11/attention/self/value/kernel:0', 'bert/encoder/layer_._1/attention/self/value/bias:0', 'mlm___cls/predictions/transform/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/attention/self/key/bias:0', 'bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/intermediate/dense/bias:0', 'bert/encoder/layer_._1/output/dense/bias:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._4/attention/self/value/kernel:0', 'mlm___cls/predictions/transform/LayerNorm/beta:0', 'bert/encoder/layer_._11/attention/self/value/bias:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/output/dense/bias:0', 'bert/encoder/layer_._9/attention/self/value/bias:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/attention/self/value/bias:0', 'bert/embeddings/word_embeddings/weight:0', 'bert/encoder/layer_._9/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/attention/self/value/kernel:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1950_1969.lowercase_64batch_size_20000steps were not used when initializing TFBertForMaskedLM: ['encoder/layer_._2/intermediate/dense/bias:0', 'encoder/layer_._3/attention/output/dense/bias:0', 'encoder/layer_._5/attention/self/query/kernel:0', 'encoder/layer_._2/output/LayerNorm/gamma:0', 'encoder/layer_._6/intermediate/dense/bias:0', 'encoder/layer_._10/attention/self/value/bias:0', 'predictions/transform/LayerNorm/beta:0', 'embeddings/token_type_embeddings/embeddings:0', 'encoder/layer_._7/output/dense/kernel:0', 'encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'encoder/layer_._2/attention/self/key/bias:0', 'encoder/layer_._5/output/LayerNorm/beta:0', 'encoder/layer_._9/intermediate/dense/kernel:0', 'encoder/layer_._8/attention/self/query/bias:0', 'encoder/layer_._3/output/LayerNorm/beta:0', 'encoder/layer_._5/intermediate/dense/bias:0', 'encoder/layer_._9/attention/self/query/bias:0', 'encoder/layer_._0/attention/self/value/kernel:0', 'encoder/layer_._7/output/LayerNorm/gamma:0', 'encoder/layer_._2/output/dense/bias:0', 'encoder/layer_._4/attention/output/dense/bias:0', 'predictions/transform/dense/bias:0', 'encoder/layer_._0/attention/output/LayerNorm/beta:0', 'encoder/layer_._10/attention/output/dense/bias:0', 'encoder/layer_._10/intermediate/dense/kernel:0', 'encoder/layer_._11/output/dense/kernel:0', 'encoder/layer_._2/output/LayerNorm/beta:0', 'encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/attention/self/key/kernel:0', 'encoder/layer_._1/attention/self/query/bias:0', 'encoder/layer_._2/attention/self/query/kernel:0', 'encoder/layer_._3/attention/self/query/bias:0', 'encoder/layer_._10/intermediate/dense/bias:0', 'encoder/layer_._0/intermediate/dense/bias:0', 'encoder/layer_._2/output/dense/kernel:0', 'encoder/layer_._5/attention/self/value/bias:0', 'encoder/layer_._5/output/dense/bias:0', 'encoder/layer_._2/attention/output/LayerNorm/beta:0', 'encoder/layer_._5/output/LayerNorm/gamma:0', 'encoder/layer_._2/attention/self/key/kernel:0', 'encoder/layer_._8/output/dense/bias:0', 'encoder/layer_._8/intermediate/dense/bias:0', 'encoder/layer_._2/attention/output/dense/kernel:0', 'encoder/layer_._9/attention/self/value/kernel:0', 'encoder/layer_._5/attention/self/query/bias:0', 'encoder/layer_._4/intermediate/dense/bias:0', 'predictions/transform/dense/kernel:0', 'encoder/layer_._1/output/dense/kernel:0', 'encoder/layer_._11/intermediate/dense/kernel:0', 'encoder/layer_._0/attention/self/query/kernel:0', 'encoder/layer_._7/attention/self/value/bias:0', 'encoder/layer_._9/intermediate/dense/bias:0', 'embeddings/LayerNorm/beta:0', 'encoder/layer_._3/intermediate/dense/bias:0', 'encoder/layer_._5/attention/self/key/bias:0', 'encoder/layer_._9/attention/output/dense/bias:0', 'encoder/layer_._6/attention/self/query/kernel:0', 'encoder/layer_._10/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/output/LayerNorm/beta:0', 'encoder/layer_._11/output/LayerNorm/beta:0', 'encoder/layer_._10/attention/self/query/kernel:0', 'encoder/layer_._2/attention/output/dense/bias:0', 'encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'encoder/layer_._8/attention/output/dense/kernel:0', 'encoder/layer_._8/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/self/key/bias:0', 'encoder/layer_._7/output/dense/bias:0', 'encoder/layer_._1/attention/output/LayerNorm/beta:0', 'encoder/layer_._8/attention/self/key/kernel:0', 'encoder/layer_._11/attention/output/dense/bias:0', 'encoder/layer_._11/output/LayerNorm/gamma:0', 'embeddings/LayerNorm/gamma:0', 'encoder/layer_._7/attention/self/query/bias:0', 'encoder/layer_._4/attention/output/LayerNorm/beta:0', 'embeddings/position_embeddings/embeddings:0', 'encoder/layer_._4/attention/self/key/bias:0', 'encoder/layer_._1/attention/self/value/bias:0', 'encoder/layer_._10/output/dense/bias:0', 'encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'encoder/layer_._5/attention/output/dense/kernel:0', 'encoder/layer_._9/attention/self/query/kernel:0', 'encoder/layer_._1/attention/self/value/kernel:0', 'encoder/layer_._1/attention/self/key/kernel:0', 'encoder/layer_._4/attention/output/dense/kernel:0', 'encoder/layer_._7/attention/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/output/dense/kernel:0', 'encoder/layer_._0/attention/output/dense/bias:0', 'encoder/layer_._3/attention/self/value/bias:0', 'encoder/layer_._3/attention/output/dense/kernel:0', 'encoder/layer_._5/output/dense/kernel:0', 'encoder/layer_._2/attention/self/value/bias:0', 'encoder/layer_._6/output/dense/kernel:0', 'encoder/layer_._10/attention/self/key/bias:0', 'encoder/layer_._0/output/LayerNorm/gamma:0', 'encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'encoder/layer_._3/output/dense/kernel:0', 'encoder/layer_._4/output/LayerNorm/beta:0', 'encoder/layer_._5/attention/self/value/kernel:0', 'encoder/layer_._1/intermediate/dense/kernel:0', 'encoder/layer_._11/attention/self/key/bias:0', 'encoder/layer_._2/intermediate/dense/kernel:0', 'embeddings/word_embeddings/weight:0', 'encoder/layer_._4/attention/self/key/kernel:0', 'encoder/layer_._3/attention/self/query/kernel:0', 'encoder/layer_._9/attention/output/LayerNorm/beta:0', 'encoder/layer_._9/attention/self/value/bias:0', 'encoder/layer_._3/output/dense/bias:0', 'encoder/layer_._7/attention/output/dense/bias:0', 'encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'encoder/layer_._1/attention/self/key/bias:0', 'encoder/layer_._6/output/dense/bias:0', 'encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'encoder/layer_._7/intermediate/dense/bias:0', 'encoder/layer_._5/attention/output/dense/bias:0', 'encoder/layer_._1/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/self/query/kernel:0', 'encoder/layer_._8/attention/self/value/bias:0', 'encoder/layer_._6/attention/self/key/bias:0', 'encoder/layer_._1/attention/self/query/kernel:0', 'encoder/layer_._10/output/LayerNorm/gamma:0', 'encoder/layer_._4/attention/self/query/bias:0', 'encoder/layer_._0/intermediate/dense/kernel:0', 'encoder/layer_._8/attention/output/LayerNorm/beta:0', 'encoder/layer_._11/attention/self/key/kernel:0', 'encoder/layer_._4/output/dense/kernel:0', 'encoder/layer_._0/attention/self/key/kernel:0', 'encoder/layer_._6/attention/self/query/bias:0', 'encoder/layer_._1/attention/output/dense/bias:0', 'encoder/layer_._0/output/dense/bias:0', 'encoder/layer_._6/attention/output/dense/bias:0', 'encoder/layer_._6/output/LayerNorm/gamma:0', 'encoder/layer_._5/attention/output/LayerNorm/beta:0', 'encoder/layer_._0/output/dense/kernel:0', 'predictions/bias:0', 'encoder/layer_._10/attention/output/dense/kernel:0', 'encoder/layer_._7/attention/self/key/kernel:0', 'encoder/layer_._7/attention/self/value/kernel:0', 'predictions/transform/LayerNorm/gamma:0', 'encoder/layer_._4/output/LayerNorm/gamma:0', 'encoder/layer_._0/attention/self/value/bias:0', 'encoder/layer_._1/output/dense/bias:0', 'encoder/layer_._8/attention/self/query/kernel:0', 'encoder/layer_._5/attention/self/key/kernel:0', 'encoder/layer_._1/output/LayerNorm/gamma:0', 'encoder/layer_._4/attention/self/value/kernel:0', 'encoder/layer_._3/attention/self/key/bias:0', 'encoder/layer_._8/attention/self/value/kernel:0', 'encoder/layer_._6/attention/self/value/bias:0', 'encoder/layer_._10/attention/self/key/kernel:0', 'encoder/layer_._7/attention/self/query/kernel:0', 'encoder/layer_._9/output/dense/bias:0', 'encoder/layer_._11/attention/self/query/bias:0', 'encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'encoder/layer_._9/attention/self/key/kernel:0', 'encoder/layer_._7/intermediate/dense/kernel:0', 'encoder/layer_._8/attention/self/key/bias:0', 'encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/attention/self/value/kernel:0', 'encoder/layer_._11/attention/self/value/kernel:0', 'encoder/layer_._4/output/dense/bias:0', 'encoder/layer_._6/attention/output/LayerNorm/beta:0', 'encoder/layer_._5/intermediate/dense/kernel:0', 'encoder/layer_._0/attention/self/query/bias:0', 'encoder/layer_._6/output/LayerNorm/beta:0', 'encoder/layer_._7/attention/output/dense/kernel:0', 'encoder/layer_._1/attention/output/dense/kernel:0', 'encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'encoder/layer_._11/attention/self/query/kernel:0', 'encoder/layer_._0/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/self/value/bias:0', 'encoder/layer_._11/intermediate/dense/bias:0', 'encoder/layer_._0/attention/self/key/bias:0', 'encoder/layer_._2/attention/self/query/bias:0', 'encoder/layer_._9/output/dense/kernel:0', 'encoder/layer_._8/output/LayerNorm/gamma:0', 'encoder/layer_._10/output/dense/kernel:0', 'encoder/layer_._2/attention/self/value/kernel:0', 'encoder/layer_._3/intermediate/dense/kernel:0', 'encoder/layer_._7/output/LayerNorm/beta:0', 'encoder/layer_._0/output/LayerNorm/beta:0', 'encoder/layer_._8/attention/output/dense/bias:0', 'encoder/layer_._11/output/dense/bias:0', 'encoder/layer_._8/output/dense/kernel:0', 'encoder/layer_._11/attention/self/value/bias:0', 'encoder/layer_._9/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/self/query/bias:0', 'encoder/layer_._3/attention/self/value/kernel:0', 'encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/output/LayerNorm/beta:0', 'encoder/layer_._8/intermediate/dense/kernel:0', 'encoder/layer_._7/attention/self/key/bias:0', 'encoder/layer_._3/attention/self/key/kernel:0', 'encoder/layer_._3/output/LayerNorm/gamma:0', 'encoder/layer_._10/attention/self/value/kernel:0', 'encoder/layer_._1/intermediate/dense/bias:0', 'encoder/layer_._4/intermediate/dense/kernel:0', 'encoder/layer_._9/output/LayerNorm/beta:0', 'encoder/layer_._3/attention/output/LayerNorm/beta:0', 'encoder/layer_._6/attention/output/dense/kernel:0', 'encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'encoder/layer_._6/intermediate/dense/kernel:0']\n",
            "- This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForMaskedLM were not initialized from the model checkpoint at /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1950_1969.lowercase_64batch_size_20000steps and are newly initialized: ['bert/encoder/layer_._10/attention/output/dense/bias:0', 'bert/encoder/layer_._0/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/output/dense/kernel:0', 'bert/encoder/layer_._10/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/self/query/bias:0', 'bert/encoder/layer_._8/attention/self/key/kernel:0', 'bert/encoder/layer_._11/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/self/query/bias:0', 'bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/intermediate/dense/kernel:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/self/query/bias:0', 'bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/output/dense/bias:0', 'bert/encoder/layer_._0/attention/self/key/bias:0', 'bert/encoder/layer_._1/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/attention/self/key/kernel:0', 'bert/encoder/layer_._4/attention/output/dense/bias:0', 'bert/encoder/layer_._3/output/dense/bias:0', 'bert/encoder/layer_._9/output/dense/bias:0', 'bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/intermediate/dense/kernel:0', 'bert/encoder/layer_._0/attention/self/key/kernel:0', 'bert/encoder/layer_._5/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/output/dense/bias:0', 'bert/encoder/layer_._7/attention/self/query/bias:0', 'bert/encoder/layer_._10/attention/self/value/bias:0', 'bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/query/kernel:0', 'bert/encoder/layer_._8/attention/output/dense/bias:0', 'bert/encoder/layer_._9/attention/self/key/kernel:0', 'bert/encoder/layer_._3/attention/self/value/kernel:0', 'bert/encoder/layer_._6/intermediate/dense/kernel:0', 'bert/encoder/layer_._9/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/intermediate/dense/kernel:0', 'bert/encoder/layer_._2/intermediate/dense/kernel:0', 'bert/encoder/layer_._1/attention/self/key/bias:0', 'bert/encoder/layer_._3/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/key/bias:0', 'bert/encoder/layer_._3/attention/output/dense/bias:0', 'bert/encoder/layer_._7/attention/output/dense/bias:0', 'bert/encoder/layer_._11/attention/self/key/bias:0', 'bert/encoder/layer_._5/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/output/dense/kernel:0', 'bert/encoder/layer_._5/output/dense/kernel:0', 'bert/encoder/layer_._10/attention/self/query/kernel:0', 'bert/encoder/layer_._5/attention/output/dense/bias:0', 'bert/encoder/layer_._5/attention/self/value/kernel:0', 'bert/encoder/layer_._10/intermediate/dense/kernel:0', 'bert/encoder/layer_._4/attention/self/key/bias:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/key/bias:0', 'bert/encoder/layer_._4/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/intermediate/dense/kernel:0', 'bert/encoder/layer_._1/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/self/key/kernel:0', 'bert/encoder/layer_._7/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/attention/self/value/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/kernel:0', 'bert/encoder/layer_._3/attention/self/key/bias:0', 'bert/encoder/layer_._1/attention/self/key/kernel:0', 'bert/encoder/layer_._4/attention/self/key/kernel:0', 'bert/encoder/layer_._4/output/dense/kernel:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/attention/self/query/kernel:0', 'bert/embeddings/position_embeddings/weight:0', 'bert/encoder/layer_._2/attention/self/value/kernel:0', 'bert/encoder/layer_._6/intermediate/dense/bias:0', 'bert/encoder/layer_._8/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/self/value/bias:0', 'bert/encoder/layer_._1/intermediate/dense/kernel:0', 'bert/encoder/layer_._10/attention/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/output/dense/bias:0', 'bert/encoder/layer_._3/attention/self/query/kernel:0', 'bert/encoder/layer_._7/attention/self/value/kernel:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/self/query/kernel:0', 'bert/encoder/layer_._0/attention/self/query/kernel:0', 'bert/encoder/layer_._10/output/dense/bias:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/output/dense/bias:0', 'bert/encoder/layer_._1/attention/self/value/kernel:0', 'bert/encoder/layer_._8/output/dense/bias:0', 'bert/encoder/layer_._5/intermediate/dense/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/bias:0', 'bert/encoder/layer_._2/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/self/query/kernel:0', 'bert/embeddings/token_type_embeddings/weight:0', 'bert/encoder/layer_._6/attention/self/key/kernel:0', 'bert/encoder/layer_._3/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/attention/self/value/bias:0', 'bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'bert/embeddings/LayerNorm/beta:0', 'bert/encoder/layer_._4/attention/self/value/bias:0', 'bert/encoder/layer_._10/attention/self/key/bias:0', 'bert/encoder/layer_._0/attention/output/dense/bias:0', 'bert/encoder/layer_._11/output/dense/bias:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/self/key/kernel:0', 'bert/encoder/layer_._11/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/query/kernel:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/self/key/bias:0', 'bert/encoder/layer_._7/output/dense/kernel:0', 'bert/encoder/layer_._10/intermediate/dense/bias:0', 'bert/encoder/layer_._1/output/dense/kernel:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/attention/self/key/bias:0', 'bert/encoder/layer_._7/attention/self/key/kernel:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/value/kernel:0', 'bert/encoder/layer_._2/attention/self/query/kernel:0', 'bert/encoder/layer_._6/attention/self/query/bias:0', 'bert/encoder/layer_._11/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/output/dense/kernel:0', 'bert/encoder/layer_._7/intermediate/dense/bias:0', 'bert/encoder/layer_._10/attention/self/key/kernel:0', 'bert/encoder/layer_._9/attention/output/dense/bias:0', 'mlm___cls/predictions/transform/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/query/bias:0', 'bert/encoder/layer_._0/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/value/bias:0', 'bert/encoder/layer_._10/attention/self/query/bias:0', 'bert/encoder/layer_._10/attention/self/value/kernel:0', 'bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/intermediate/dense/bias:0', 'bert/encoder/layer_._8/attention/output/dense/kernel:0', 'bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/output/dense/bias:0', 'bert/encoder/layer_._4/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/query/kernel:0', 'bert/encoder/layer_._3/output/dense/kernel:0', 'bert/encoder/layer_._11/attention/self/query/bias:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'mlm___cls/predictions/bias:0', 'bert/encoder/layer_._8/output/dense/kernel:0', 'bert/encoder/layer_._9/intermediate/dense/bias:0', 'mlm___cls/predictions/transform/dense/bias:0', 'bert/encoder/layer_._2/attention/self/value/bias:0', 'bert/encoder/layer_._8/intermediate/dense/bias:0', 'bert/encoder/layer_._3/attention/self/key/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/self/query/kernel:0', 'bert/encoder/layer_._0/output/dense/bias:0', 'bert/encoder/layer_._8/attention/self/key/bias:0', 'bert/encoder/layer_._6/attention/output/dense/bias:0', 'bert/encoder/layer_._8/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/output/dense/bias:0', 'bert/encoder/layer_._6/attention/self/query/kernel:0', 'bert/encoder/layer_._5/attention/self/query/bias:0', 'bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'bert/encoder/layer_._3/intermediate/dense/bias:0', 'bert/encoder/layer_._2/output/dense/bias:0', 'bert/embeddings/LayerNorm/gamma:0', 'bert/encoder/layer_._5/intermediate/dense/bias:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._8/attention/self/value/kernel:0', 'bert/encoder/layer_._11/attention/self/value/kernel:0', 'bert/encoder/layer_._1/attention/self/value/bias:0', 'mlm___cls/predictions/transform/LayerNorm/gamma:0', 'bert/encoder/layer_._11/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/beta:0', 'bert/encoder/layer_._6/attention/self/key/bias:0', 'bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/intermediate/dense/bias:0', 'bert/encoder/layer_._1/output/dense/bias:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._4/attention/self/value/kernel:0', 'mlm___cls/predictions/transform/LayerNorm/beta:0', 'bert/encoder/layer_._11/attention/self/value/bias:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/output/dense/bias:0', 'bert/encoder/layer_._9/attention/self/value/bias:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/attention/self/value/bias:0', 'bert/embeddings/word_embeddings/weight:0', 'bert/encoder/layer_._9/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/attention/self/value/kernel:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "e1 = pickle.load(open(os.path.join(PROJECT_DIR,'embeddings1.pickle'), 'rb'))\n"
      ],
      "metadata": {
        "id": "N97iHuO-p9GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k1, _ = get_k_nearest_neighbors(\n",
        "    token,\n",
        "    bert_model1,\n",
        "    embeddings1,\n",
        "    tokenizer1,\n",
        "    k = k\n",
        ")"
      ],
      "metadata": {
        "id": "b7iSSR0dqH8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving Vocab from each Tokenizer"
      ],
      "metadata": {
        "id": "jwe4REPAXxZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1 = tokenizer1.get_vocab()\n",
        "embeddings1 = [get_token_embedding(token, bert_model1, tokenizer1) for token in vocab1.keys()]\n",
        "print(f\"Retrieved embeddings for {MODEL1_PATH}\")\n",
        "\n",
        "vocab2 = tokenizer2.get_vocab()\n",
        "embeddings2 = [get_token_embedding(token, bert_model2, tokenizer2) for token in vocab2.keys()]\n",
        "print(f\"Retrieved embeddings for {MODEL2_PATH}\")"
      ],
      "metadata": {
        "id": "vDoHCPJoUPe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8054a4c6-0427-46ee-ed02-676b521b8ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved embeddings for birthyear.1990_2009.lowercase_64batch_size_20000steps\n",
            "Retrieved embeddings for birthyear.1950_1969.lowercase_64batch_size_20000steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(os.path.join(PROJECT_DIR,'embeddings1.pickle'), 'wb') as handle:\n",
        "    pickle.dump(embeddings1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(os.path.join(PROJECT_DIR,'embeddings2.pickle'), 'wb') as handle:\n",
        "    pickle.dump(embeddings2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "LwBMMz9AwNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing to Tokens from Original Paper"
      ],
      "metadata": {
        "id": "8o1bMEVRctXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"dem\", \"dam\", \"rep\", \"assist\", \"pr\", \"fr\", \"joint\", \"mega\", \"flow\", \"icymi\"]\n",
        "k = 10\n",
        "for token in tokens:\n",
        "    top_k1, _ = get_k_nearest_neighbors(\n",
        "        token,\n",
        "        bert_model1,\n",
        "        embeddings1,\n",
        "        tokenizer1,\n",
        "        k = k\n",
        "    )\n",
        "\n",
        "    top_k2, _ = get_k_nearest_neighbors(\n",
        "        token,\n",
        "        bert_model2,\n",
        "        embeddings2,\n",
        "        tokenizer2,\n",
        "        k = k\n",
        "    )\n",
        "\n",
        "    print(f\"Token: {token}\")\n",
        "    print(f\"Top {k} neighbors from tokenizer 1:\")\n",
        "    print(top_k1)\n",
        "    print(f\"Top {k} neighbors from tokenizer 2:\")\n",
        "    print(top_k2)\n",
        "    intersection = set(top_k1).intersection(set(top_k2))\n",
        "    print(f\"Intersection: {intersection} ({len(intersection)} / {k})\")\n",
        "    print(\"========================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZyvR8JwY4ip",
        "outputId": "e07e229c-f906-4b86-9a8e-00de96af4300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: dem\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['concentrate' 'blueberry' 'releg' 'ipo' 'bankruptcy' 'tko' 'worms'\n",
            " 'interact' 'discom' 'prolly']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['logging' 'ense' 'deplorable' 'slo' 'gulf' 'disastrous' 'doj' 'marvelous'\n",
            " 'slaves' 'intelligence']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: dam\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['gorg' '📔' 'attendants' 'bearded' 'punishment' 'ballroom' 'entitlement'\n",
            " 'blasting' 'contempor' 'prod']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['applauded' 'tina' 'piggy' 'wreat' '🧒' 'preced' 'article' 'gomez'\n",
            " 'restore' '泰']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: rep\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['flattered' 'amendment' 'kicker' '##💭' 'swedish' 'performing'\n",
            " 'disappointment' 'captaincy' 'bullied' 'steeler']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['wild' 'infinitely' 'illusion' 'merge' 'deemed' 'residence' 'azerbai'\n",
            " 'journos' 'juicy' 'orthodox']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: assist\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['frankly' 'bord' 'cem' 'alle' 'documents' 'racquet' 'struggled' 'chch'\n",
            " 'protestors' 'fractured']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['蕉' 'homework' 'gutted' 'gene' 'brutally' 'skies' 'impl' 'с' 'patio'\n",
            " 'listens']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: pr\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['bikes' 'britain' 'overs' 'individually' 'hallmark' 'puff' 'bearded'\n",
            " 'adaptive' 'wiz' '🚦']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['phenom' 'stations' 'loads' 'reli' 'consol' '🔓' 'sever' 'brill' 'tah'\n",
            " 'jody']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: fr\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['listeners' 'os' 'verse' 'gestures' 'snowfl' 'boooom' '👲' 'thatcher'\n",
            " 'flood' 'akron']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['voter' 'juicy' 'barrett' 'retain' 'automob' 'bella' 'riverside' 'slip'\n",
            " 'poem' '宙']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: joint\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['establish' 'jawn' 'valued' 'shhhh' 'lotta' 'shrek' 'unconditional' 'ipo'\n",
            " 'seemed' 'citizen']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['fulfilled' 'iii' 'billie' 'lord' 'influencing' 'dreaded' 'absurdity'\n",
            " 'colleges' 'strawberries' 'embarrass']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: mega\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['submissions' 'parker' 'cousins' 'tlc' 'transfer' 'hil' 'fairytale' 'fa'\n",
            " 'accum' 'unint']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['guitar' 'schedul' 'molly' 'adventures' 'sealed' 'tails' 'sooners'\n",
            " 'letters' 'attend' 'rebu']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: flow\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['huskies' 'virtue' 'unc' '🐧' 'musicians' 'attem' 'bian' 'hawthorn'\n",
            " 'uplifting' 'malta']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['kimmel' 'mathematic' 'backward' 'married' 'mcmaster' 'hou' 'attr' 'mov'\n",
            " 'say' 'youngest']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n",
            "Token: icymi\n",
            "Top 10 neighbors from tokenizer 1:\n",
            "['whistler' 'chance' 'pon' 'moes' 'young' 'smitty' 'extra' 'cammy'\n",
            " 'subtweet' 'cuppa']\n",
            "Top 10 neighbors from tokenizer 2:\n",
            "['DIGIT' 'faves' 'staged' 'gq' 'plays' '😘' 'sheila' 'dreaded' 'wanda'\n",
            " 'power']\n",
            "Intersection: set() (0 / 10)\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating on a Pre-Trained BERT Model"
      ],
      "metadata": {
        "id": "VZ3vnkuuvMjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on a Pre-trained BERT Model\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "embeddings = [get_embedding(token, model, tokenizer) for token in vocab.keys()]\n",
        "\n",
        "top_k, _ = get_k_nearest_neighbors(\n",
        "    \"Australia\",\n",
        "    model,\n",
        "    embeddings,\n",
        "    tokenizer,\n",
        "    k = 10,\n",
        "    framework = 'tf'\n",
        ")\n",
        "top_k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfTE2n1zGlSW",
        "outputId": "9ac4a3b7-58f8-448f-b0fd-2b19f9c4a746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdBdK0j1epzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}