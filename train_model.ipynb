{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request, json, os\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig, \n",
    "    TFBertForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AdamWeightDecay\n",
    ")\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    processors,\n",
    "    decoders\n",
    ")\n",
    "\n",
    "from datasets import IterableDataset, load_dataset, load_from_disk\n",
    "\n",
    "from utils import (\n",
    "    TweetIterator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "DATA_DIR = 'data'\n",
    "TWEETS_PATH = 'birthyear.1950_1969.lowercase'\n",
    "\n",
    "full_tweets_path = os.path.join(DATA_DIR, TWEETS_PATH)\n",
    "dataset_path = '.'.join(TWEETS_PATH.split('.')[:-1]) + '.hf'\n",
    "full_dataset_path = os.path.join(DATA_DIR, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Untrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  86218752  \n",
      "                                                                 \n",
      " mlm___cls (TFBertMLMHead)   multiple                  1757416   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86811880 (331.16 MB)\n",
      "Trainable params: 86811880 (331.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/W266/lib/python3.12/site-packages/tf_keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# not needed - BertConfig default settings are equivalent to bert-base-uncased config\n",
    "# with urllib.request.urlopen(\"https://huggingface.co/google-bert/bert-base-uncased/resolve/main/config.json\") as url:\n",
    "#     config_from_pretrained = json.load(url) # bert-base-uncased config\n",
    "    \n",
    "# initialize & build Masked LM BERT model w/ default config settings\n",
    "config = BertConfig(\n",
    "    vocab_size = VOCAB_SIZE\n",
    ")\n",
    "bert_model = TFBertForMaskedLM(config = config)\n",
    "bert_model.build()\n",
    "bert_model.compile(optimizer = AdamWeightDecay(lr=2e-5, weight_decay_rate=0.01))\n",
    "\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train BERT Tokenizer\n",
    "https://huggingface.co/learn/nlp-course/en/chapter6/8#building-a-wordpiece-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BERT tokenizer\n",
    "tokenizer = Tokenizer(model = models.WordPiece(unk_token = '[UNK]'))\n",
    "normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),            # Normalize characters to Unicode NFD\n",
    "    # normalizers.Lowercase(),      # Set all characters to lowercase - not necessary, as tweets are already lowercase\n",
    "    normalizers.StripAccents()    # Remove all accents from characters\n",
    "])\n",
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# create a WordPiece trainer\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    special_tokens = special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create an iterator out of our tweets since they won't fit on disk\n",
    "tweets = TweetIterator(full_tweets_path)\n",
    "tokenizer.train_from_iterator(tweets, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", tokenizer.token_to_id('[CLS]')), (\"[SEP]\", tokenizer.token_to_id('[SEP]'))],\n",
    ")\n",
    "tokenizer.post_processor = post_processor\n",
    "\n",
    "decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap our tokenizer in a PreTrainedTokenizerFast object\n",
    "# so we can use it with a DataCollator and Trainer\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Datasets from Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = wrapped_tokenizer, \n",
    "    mlm_probability = 0.15, # probability that a token is masked\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've saved this dataset before at data/birthyear.1950_1969.hf.\n",
      "Loading from disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n"
     ]
    }
   ],
   "source": [
    "# TODO - wrap this in a function\n",
    "if dataset_path in os.listdir(os.path.join(DATA_DIR, os.path.curdir)):\n",
    "    print(f\"You've saved this dataset before at {full_dataset_path}.\")\n",
    "    print(\"Loading from disk...\")\n",
    "    tweets_ds = load_from_disk(full_dataset_path)\n",
    "    print(\"Dataset loaded!\")\n",
    "else:\n",
    "    print(f\"You've never saved this dataset before. Loading dataset from {full_tweets_path}...\")\n",
    "    tweets_ds = load_dataset(\n",
    "        path = \"text\",\n",
    "        data_files = full_tweets_path,\n",
    "        split = f\"train\", # train on everything\n",
    "    )\n",
    "    print('Tokenizing dataset...')\n",
    "    tweets_ds = tweets_ds.map(\n",
    "        function = lambda x: wrapped_tokenizer(x['text']),\n",
    "        batched = True\n",
    "    )\n",
    "    print('Saving dataset to disk...')\n",
    "    tweets_ds.save_to_disk(full_dataset_path)\n",
    "    print(f'Dataset saved at {full_dataset_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A SMALL DATASET FOR TESTING\n",
    "# print(f\"You've never saved this dataset before. Loading dataset from {full_tweets_path}...\")\n",
    "# tweets_ds = load_dataset(\n",
    "#     path = \"text\",\n",
    "#     data_files = full_tweets_path,\n",
    "#     split = f\"train[:5%]\", # train on everything\n",
    "# )\n",
    "# print('Tokenizing dataset...')\n",
    "# tweets_ds = tweets_ds.map(\n",
    "#     function = lambda x: wrapped_tokenizer(x['text']),\n",
    "#     batched = True\n",
    "# )\n",
    "# print('Saving dataset to disk...')\n",
    "# small_dataset_path = 'data/birthyear.1950_1969_SMALL.hf'\n",
    "# tweets_ds.save_to_disk(small_dataset_path)\n",
    "# print(f'Dataset saved at {small_dataset_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ds_tf = bert_model.prepare_tf_dataset(\n",
    "    dataset = tweets_ds,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x16beb14e0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x16beb14e0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "bert_model.fit(\n",
    "    x = tweets_ds_tf, \n",
    "    epochs = 1,\n",
    "    steps_per_epoch = 1,\n",
    "    verbose = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
