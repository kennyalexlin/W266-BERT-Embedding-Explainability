{"cells":[{"cell_type":"code","source":["# if in Google colab, need to install datasets\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZH68AL__wor","executionInfo":{"status":"ok","timestamp":1722220189051,"user_tz":300,"elapsed":9048,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"9c73a412-6310-46c1-afc7-54ff1162fef6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.6.1\n","    Uninstalling fsspec-2024.6.1:\n","      Successfully uninstalled fsspec-2024.6.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"jVxh_6-SssL3","executionInfo":{"status":"ok","timestamp":1722220389260,"user_tz":300,"elapsed":15157,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import urllib.request, json, os, math\n","\n","from transformers import (\n","    BertConfig,\n","    TFBertForMaskedLM,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments,\n","    PreTrainedTokenizerFast,\n","    AdamWeightDecay,\n","    create_optimizer\n",")\n","\n","from tokenizers import (\n","    Tokenizer,\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    trainers,\n","    processors,\n","    decoders\n",")\n","\n","from datasets import IterableDataset, load_dataset, load_from_disk"]},{"cell_type":"code","source":["try:\n","    from google.colab import drive\n","    import sys\n","\n","    drive.mount('/content/gdrive/')\n","    sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n","except:\n","    pass\n","\n","from utils import (\n","    TweetIterator\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDFUrpqVNUSd","executionInfo":{"status":"ok","timestamp":1722220390720,"user_tz":300,"elapsed":1489,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"18e6cadc-6e8f-4be8-f0a2-c336e5ff4a18"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"zjCD-Sp0ssL6"},"source":["## Config"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4SHXwYd2ssL6","executionInfo":{"status":"ok","timestamp":1722220390721,"user_tz":300,"elapsed":4,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}}},"outputs":[],"source":["PROJECT_DIR = '/content/gdrive/My Drive/Colab Notebooks/w266_final_proj' # filepath to store model/tokenizer/data artifacts\n","TWEETS_PATH = 'birthyear.1950_1969.lowercase'                            # name of tweets file\n","VOCAB_SIZE = 30522                                                       # same as vocab size in BERT paper\n","TEST_PCT = 0.1                                                           # defines pct of total dataset to use for validation\n","TRAIN_BATCH_SIZE = 64                                                    # batch size for training\n","TEST_BATCH_SIZE = 32                                                     # batch size for validation\n","INITIAL_LEARNING_RATE = 0.0001                                           # LR to use @ during warmup learning schedule, same as BERT paper\n","WEIGHT_DECAY = 0.01                                                      # Regularization weight, same as BERT paper\n","TOTAL_NUM_STEPS = 1000                                                   # number of batches to use for training\n","\n","# path to raw tweet data\n","full_tweets_path = os.path.join(PROJECT_DIR, TWEETS_PATH)\n","\n","# path to save or load pre-processed tweets dataset (will load if pre-processed dataset already exists)\n","dataset_path = '.'.join(TWEETS_PATH.split('.')[:-1]) + '.hf'\n","full_dataset_path = os.path.join(PROJECT_DIR, dataset_path)\n","\n","# path to save or load tokenizer to (will load if pre-trained tokenizer already exists)\n","tokenizer_path = TWEETS_PATH + '_tokenizer'\n","full_tokenizer_path = os.path.join(PROJECT_DIR, tokenizer_path)\n","\n","# path to save BERT model to\n","model_path = f'{TWEETS_PATH}_{TRAIN_BATCH_SIZE}batch_size_{TOTAL_NUM_STEPS}steps'\n","full_model_path = os.path.join(PROJECT_DIR, model_path)"]},{"cell_type":"code","source":["num_tweets = 0\n","with open(full_tweets_path, 'r') as file:\n","    for line in file:\n","        num_tweets += 1\n","print(f\"Total Number of Tweets: {num_tweets:,.0f}\")\n","\n","num_training_tweets = TRAIN_BATCH_SIZE * TOTAL_NUM_STEPS\n","print(f\"Based on the selected parameters, model will be trained on {num_training_tweets / num_tweets :.1%} of total Tweets ({num_training_tweets:,.0f})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMYj9IsZw74Q","executionInfo":{"status":"ok","timestamp":1722220403201,"user_tz":300,"elapsed":8769,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"fdc1c777-1c4d-484f-c62d-2f85a0edcb2b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Number of Tweets: 8,167,178\n","Based on the selected parameters, model will be trained on 0.8% of total Tweets (64,000)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rtXVtr-PssL7"},"source":["### Create Untrained BERT Model"]},{"cell_type":"markdown","metadata":{"id":"EYytzZ2HssL7"},"source":["### Create and Train BERT Tokenizer\n","https://huggingface.co/learn/nlp-course/en/chapter6/8#building-a-wordpiece-tokenizer-from-scratch"]},{"cell_type":"code","source":["if tokenizer_path in os.listdir(PROJECT_DIR):\n","    print(f\"You've saved this tokenizer before at {full_tokenizer_path}.\")\n","    print(\"Loading from disk...\")\n","\n","    wrapped_tokenizer = PreTrainedTokenizerFast.from_pretrained(full_tokenizer_path)\n","    print(\"Tokenizer loaded!\")\n","else:\n","    print(\"You've never saved this dataset before. Creating a tokenizer from scratch...\")\n","    # create a BERT tokenizer\n","    tokenizer = Tokenizer(model = models.WordPiece(unk_token = '[UNK]'))\n","    normalizer = normalizers.Sequence([\n","        normalizers.NFD(),            # Normalize characters to Unicode NFD\n","        # normalizers.Lowercase(),      # Set all characters to lowercase - not necessary, as tweets are already lowercase\n","        normalizers.StripAccents()    # Remove all accents from characters\n","    ])\n","    pre_tokenizer = pre_tokenizers.Whitespace()\n","\n","    tokenizer.normalizer = normalizer\n","    tokenizer.pre_tokenizer = pre_tokenizer\n","\n","    # create a WordPiece trainer\n","    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n","    trainer = trainers.WordPieceTrainer(\n","        vocab_size = VOCAB_SIZE,\n","        special_tokens = special_tokens\n","    )\n","    print(f\"Creating an iterator from tweets at {full_tweets_path}...\")\n","    # create an iterator out of our tweets since they won't fit on disk\n","    tweets = TweetIterator(full_tweets_path)\n","    print(f\"Training your tokenizer...\")\n","    tokenizer.train_from_iterator(tweets, trainer=trainer, length=num_tweets)\n","\n","    post_processor = processors.TemplateProcessing(\n","        single=f\"[CLS]:0 $A:0 [SEP]:0\",\n","        pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n","        special_tokens=[(\"[CLS]\", tokenizer.token_to_id('[CLS]')), (\"[SEP]\", tokenizer.token_to_id('[SEP]'))]\n","    )\n","    tokenizer.post_processor = post_processor\n","\n","    decoder = decoders.WordPiece(prefix=\"##\")\n","    tokenizer.decoder = decoder\n","    # wrap our tokenizer in a PreTrainedTokenizerFast object\n","    # so we can use it with a DataCollator and Trainer\n","    wrapped_tokenizer = PreTrainedTokenizerFast(\n","        tokenizer_object=tokenizer,\n","        # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n","        unk_token=\"[UNK]\",\n","        pad_token=\"[PAD]\",\n","        cls_token=\"[CLS]\",\n","        sep_token=\"[SEP]\",\n","        mask_token=\"[MASK]\",\n","    )\n","    print('Saving tokenizer to disk...')\n","    wrapped_tokenizer.save_pretrained(full_tokenizer_path)\n","    print(f'Tokenizer saved at {full_tokenizer_path}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eT14S5S0I0nD","executionInfo":{"status":"ok","timestamp":1722220441973,"user_tz":300,"elapsed":1793,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"a7c33cf8-8987-42da-a970-577853eee0aa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["You've saved this tokenizer before at /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1950_1969.lowercase_tokenizer.\n","Loading from disk...\n","Tokenizer loaded!\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"5j5ZFN4mI67D"}},{"cell_type":"markdown","metadata":{"id":"m_n_JY-ZssL8"},"source":["## Create Train and Test Datasets from Iterator"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EXnla-ZkssL9","executionInfo":{"status":"ok","timestamp":1722220445504,"user_tz":300,"elapsed":401,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}}},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(\n","    tokenizer = wrapped_tokenizer,\n","    mlm_probability = 0.15, # probability that a token is masked\n","    return_tensors=\"tf\"\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQtzWexissL9","executionInfo":{"status":"ok","timestamp":1722220470263,"user_tz":300,"elapsed":13956,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"eb4e8831-4854-42d2-a0da-28b682f73e78"},"outputs":[{"output_type":"stream","name":"stdout","text":["You've saved this dataset before at /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1950_1969.hf.\n","Loading from disk...\n","Dataset loaded!\n"]}],"source":["if dataset_path in os.listdir(PROJECT_DIR):\n","    print(f\"You've saved this dataset before at {full_dataset_path}.\")\n","    print(\"Loading from disk...\")\n","    tweets_ds = load_from_disk(full_dataset_path)\n","    print(\"Dataset loaded!\")\n","else:\n","    print(f\"You've never saved this dataset before. Loading dataset from {full_tweets_path}...\")\n","    tweets_ds = load_dataset(\n","        path = \"text\",\n","        data_files = full_tweets_path,\n","        split = f\"train\", # train on everything\n","    )\n","    print('Tokenizing dataset...')\n","    tweets_ds = tweets_ds.map(\n","        function = lambda x: wrapped_tokenizer(x['text']),\n","        batched = True\n","    )\n","    print('Saving dataset to disk...')\n","    tweets_ds.save_to_disk(full_dataset_path)\n","    print(f'Dataset saved at {full_dataset_path}')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"62iMe0NfssL9","executionInfo":{"status":"ok","timestamp":1722220473533,"user_tz":300,"elapsed":3286,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}}},"outputs":[],"source":["# split your dataset into train and val\n","tweets_ds_split = tweets_ds.train_test_split(test_size=TEST_PCT)"]},{"cell_type":"markdown","source":["## Initialize Untrained BERT Model\n","\n"],"metadata":{"id":"AKkJVXXlLgta"}},{"cell_type":"code","source":["# Define the optimizer and learning rate schedule\n","# original BERT Model trained for 1,000,000 steps total\n","# first 1% (10,000 steps) were warm-up steps w/ static 1e-4 LR, then linear loss ratio decay\n","num_warmup_steps = math.floor(TOTAL_NUM_STEPS * 0.01)\n","\n","# linear loss ratio decay by default is from init_lr to 0 over the remaining # of steps\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=INITIAL_LEARNING_RATE,\n","    num_train_steps=TOTAL_NUM_STEPS,\n","    num_warmup_steps=num_warmup_steps,\n","    weight_decay_rate=WEIGHT_DECAY,\n",")"],"metadata":{"id":"jCD1o3MKHZLM","executionInfo":{"status":"ok","timestamp":1722220475816,"user_tz":300,"elapsed":4,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["config = BertConfig(\n","    vocab_size = VOCAB_SIZE\n",")\n","bert_model = TFBertForMaskedLM(config = config)\n","bert_model.build()\n","bert_model.compile(optimizer = optimizer)\n","\n","bert_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0acWz82Z-7Pc","executionInfo":{"status":"ok","timestamp":1722220481072,"user_tz":300,"elapsed":1777,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"51fe64ff-5690-4551-e52b-d3b9016948ac"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"tf_bert_for_masked_lm\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  108891648 \n","                                                                 \n"," mlm___cls (TFBertMLMHead)   multiple                  24459834  \n","                                                                 \n","=================================================================\n","Total params: 109514298 (417.76 MB)\n","Trainable params: 109514298 (417.76 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Prepare Dataset for Training Loop"],"metadata":{"id":"GPenkUQsLnIL"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"YWIdVEP8ssL9","executionInfo":{"status":"ok","timestamp":1722220768846,"user_tz":300,"elapsed":1289,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"560d8d65-d02e-4cee-ee40-1a47753d4941"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Train Dataset Size: 114,850 steps, although the model will only be trained on 1000 steps.\n","Total Test Dataset Size: 25,522\n"]}],"source":["tweets_ds_tf_train = bert_model.prepare_tf_dataset(\n","    dataset = tweets_ds_split[\"train\"],\n","    collate_fn = data_collator,\n","    batch_size = TRAIN_BATCH_SIZE,\n","    shuffle=True\n",")\n","\n","tweets_ds_tf_test = bert_model.prepare_tf_dataset(\n","    dataset = tweets_ds_split[\"test\"],\n","    collate_fn = data_collator,\n","    batch_size = TEST_BATCH_SIZE,\n","    shuffle=True\n",")\n","\n","print(f\"Total Train Dataset Size: {len(tweets_ds_tf_train):,.0f} steps, although the model will only be trained on {TOTAL_NUM_STEPS} steps.\")\n","print(f\"Total Test Dataset Size: {len(tweets_ds_tf_test):,.0f}\")"]},{"cell_type":"code","source":["# this number doesn't affect training since we aren't actually doing a full pass\n","# over the training set. just using to monitor model performance\n","NUM_EPOCHS = 5\n","steps_per_epoch = TOTAL_NUM_STEPS // NUM_EPOCHS\n","validation_steps = steps_per_epoch // 100\n","\n","print(f'Training Steps per Epoch: {steps_per_epoch}')\n","print(f'Validation Steps per Epoch: {validation_steps}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvnnNdyt5cK2","executionInfo":{"status":"ok","timestamp":1722220791273,"user_tz":300,"elapsed":347,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"80861891-788f-47d3-c94a-4af2d8b07038"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Steps per Epoch: 200\n","Validation Steps per Epoch: 2\n"]}]},{"cell_type":"markdown","source":["## üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è Train!"],"metadata":{"id":"kYJnyZ-R5wua"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5Fya59XssL9","executionInfo":{"status":"ok","timestamp":1722202896480,"user_tz":300,"elapsed":822325,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"49a31f29-1857-4526-a81e-9f706307ac54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","200/200 [==============================] - 218s 831ms/step - loss: 7.9412 - val_loss: 7.3218\n","Epoch 2/5\n","200/200 [==============================] - 151s 757ms/step - loss: 7.5068 - val_loss: 7.2714\n","Epoch 3/5\n","200/200 [==============================] - 150s 750ms/step - loss: 7.4537 - val_loss: 7.3789\n","Epoch 4/5\n","200/200 [==============================] - 151s 756ms/step - loss: 7.4665 - val_loss: 7.7439\n","Epoch 5/5\n","200/200 [==============================] - 150s 752ms/step - loss: 7.4370 - val_loss: 7.4751\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf_keras.src.callbacks.History at 0x7ef716689180>"]},"metadata":{},"execution_count":32}],"source":["history = bert_model.fit(\n","    x = tweets_ds_tf_train,\n","    validation_data = tweets_ds_tf_test,\n","    epochs = NUM_EPOCHS,\n","    steps_per_epoch = steps_per_epoch,\n","    validation_steps = validation_steps,\n","    verbose = 1\n",")"]},{"cell_type":"code","source":["bert_model.save_pretrained(full_model_path)\n","print(f\"Model saved to {full_model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ETVJi4UdLp5M","executionInfo":{"status":"ok","timestamp":1722202899885,"user_tz":300,"elapsed":3429,"user":{"displayName":"Kenneth Lin","userId":"01443000796591967739"}},"outputId":"8dd8f2c1-0613-48c5-be14-da5394819218"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to /content/gdrive/My Drive/Colab Notebooks/w266_final_proj/birthyear.1950_1969.lowercase_64batch_size_1000steps\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}