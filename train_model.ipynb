{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# if in Google colab, need to install datasets\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ZH68AL__wor",
        "outputId": "4cbe03f9-df6c-4088-9767-948cf301d78d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow",
                  "requests"
                ]
              },
              "id": "7c0f420260264645916694ab3df9863f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jVxh_6-SssL3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request, json, os, math\n",
        "\n",
        "from transformers import (\n",
        "    BertConfig,\n",
        "    TFBertForMaskedLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    PreTrainedTokenizerFast,\n",
        "    AdamWeightDecay,\n",
        "    create_optimizer\n",
        ")\n",
        "\n",
        "from tokenizers import (\n",
        "    Tokenizer,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    trainers,\n",
        "    processors,\n",
        "    decoders\n",
        ")\n",
        "\n",
        "from datasets import IterableDataset, load_dataset, load_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    import sys\n",
        "\n",
        "    drive.mount('/content/gdrive/')\n",
        "    sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from utils import (\n",
        "    TweetIterator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDFUrpqVNUSd",
        "outputId": "82adabb9-5f15-4f0c-e14b-a252092e4b5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjCD-Sp0ssL6"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4SHXwYd2ssL6"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 30522\n",
        "DATA_DIR = '/content/gdrive/My Drive/Colab Notebooks' # data\n",
        "TWEETS_PATH = 'birthyear.1990_2009.lowercase'         # name of tweets file\n",
        "TEST_PCT = 0.1 # defines pct of total dataset to use for validation, 1 - TEST_PCT = pct of dataset to use for training\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 16\n",
        "INITIAL_LEARNING_RATE = 0.0001 # same as init rate in BERT paper\n",
        "WEIGHT_DECAY = 0.01           # same as weight decay in BERT paper\n",
        "\n",
        "full_tweets_path = os.path.join(DATA_DIR, TWEETS_PATH)\n",
        "dataset_path = '.'.join(TWEETS_PATH.split('.')[:-1]) + '.hf'\n",
        "full_dataset_path = os.path.join(DATA_DIR, dataset_path)\n",
        "tokenizer_path = TWEETS_PATH + '_tokenizer'\n",
        "full_tokenizer_path = os.path.join(DATA_DIR, tokenizer_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tweets = 0\n",
        "with open(full_tweets_path, 'r') as file:\n",
        "    for line in file:\n",
        "        num_tweets += 1\n",
        "print(f\"Total Number of Tweets: {num_tweets:,.0f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMYj9IsZw74Q",
        "outputId": "305bbbab-70d0-4eb8-b444-ab1da6b311f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Tweets: 5,447,916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtXVtr-PssL7"
      },
      "source": [
        "### Create Untrained BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYytzZ2HssL7"
      },
      "source": [
        "### Create and Train BERT Tokenizer\n",
        "https://huggingface.co/learn/nlp-course/en/chapter6/8#building-a-wordpiece-tokenizer-from-scratch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer_path in os.listdir(os.path.join(DATA_DIR, os.path.curdir)):\n",
        "    print(f\"You've saved this tokenizer before at {full_tokenizer_path}.\")\n",
        "    print(\"Loading from disk...\")\n",
        "\n",
        "    wrapped_tokenizer = PreTrainedTokenizerFast.from_pretrained(os.path.join(DATA_DIR, tokenizer_path))\n",
        "    print(\"Tokenizer loaded!\")\n",
        "else:\n",
        "    print(\"You've never saved this dataset before. Creating a tokenizer from scratch...\")\n",
        "    # create a BERT tokenizer\n",
        "    tokenizer = Tokenizer(model = models.WordPiece(unk_token = '[UNK]'))\n",
        "    normalizer = normalizers.Sequence([\n",
        "        normalizers.NFD(),            # Normalize characters to Unicode NFD\n",
        "        # normalizers.Lowercase(),      # Set all characters to lowercase - not necessary, as tweets are already lowercase\n",
        "        normalizers.StripAccents()    # Remove all accents from characters\n",
        "    ])\n",
        "    pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "    tokenizer.normalizer = normalizer\n",
        "    tokenizer.pre_tokenizer = pre_tokenizer\n",
        "\n",
        "    # create a WordPiece trainer\n",
        "    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "    trainer = trainers.WordPieceTrainer(\n",
        "        vocab_size = VOCAB_SIZE,\n",
        "        special_tokens = special_tokens\n",
        "    )\n",
        "    print(f\"Creating an iterator from tweets at {full_tweets_path}...\")\n",
        "    # create an iterator out of our tweets since they won't fit on disk\n",
        "    tweets = TweetIterator(full_tweets_path)\n",
        "    print(f\"Training your tokenizer...\")\n",
        "    tokenizer.train_from_iterator(tweets, trainer=trainer, length=num_tweets)\n",
        "\n",
        "    post_processor = processors.TemplateProcessing(\n",
        "        single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "        pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "        special_tokens=[(\"[CLS]\", tokenizer.token_to_id('[CLS]')), (\"[SEP]\", tokenizer.token_to_id('[SEP]'))]\n",
        "    )\n",
        "    tokenizer.post_processor = post_processor\n",
        "\n",
        "    decoder = decoders.WordPiece(prefix=\"##\")\n",
        "    tokenizer.decoder = decoder\n",
        "    # wrap our tokenizer in a PreTrainedTokenizerFast object\n",
        "    # so we can use it with a DataCollator and Trainer\n",
        "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "        tokenizer_object=tokenizer,\n",
        "        # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
        "        unk_token=\"[UNK]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "    )\n",
        "    print('Saving tokenizer to disk...')\n",
        "    wrapped_tokenizer.save_pretrained(os.path.join(DATA_DIR, tokenizer_path))\n",
        "    print(f'Tokenizer saved at {full_tokenizer_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT14S5S0I0nD",
        "outputId": "7b5dcaf0-8955-4013-80bc-c6d6caad4b25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You've saved this tokenizer before at /content/gdrive/My Drive/Colab Notebooks/birthyear.1990_2009.lowercase_tokenizer.\n",
            "Loading from disk...\n",
            "Tokenizer loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5j5ZFN4mI67D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_n_JY-ZssL8"
      },
      "source": [
        "## Create Train and Test Datasets from Iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EXnla-ZkssL9"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = wrapped_tokenizer,\n",
        "    mlm_probability = 0.15, # probability that a token is masked\n",
        "    return_tensors=\"tf\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQtzWexissL9",
        "outputId": "70382b1b-37ee-42b7-fdb6-d885ed5ceacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You've saved this dataset before at /content/gdrive/My Drive/Colab Notebooks/birthyear.1990_2009.hf.\n",
            "Loading from disk...\n",
            "Dataset loaded!\n"
          ]
        }
      ],
      "source": [
        "if dataset_path in os.listdir(os.path.join(DATA_DIR, os.path.curdir)):\n",
        "    print(f\"You've saved this dataset before at {full_dataset_path}.\")\n",
        "    print(\"Loading from disk...\")\n",
        "    tweets_ds = load_from_disk(full_dataset_path)\n",
        "    print(\"Dataset loaded!\")\n",
        "else:\n",
        "    print(f\"You've never saved this dataset before. Loading dataset from {full_tweets_path}...\")\n",
        "    tweets_ds = load_dataset(\n",
        "        path = \"text\",\n",
        "        data_files = full_tweets_path,\n",
        "        split = f\"train\", # train on everything\n",
        "    )\n",
        "    print('Tokenizing dataset...')\n",
        "    tweets_ds = tweets_ds.map(\n",
        "        function = lambda x: wrapped_tokenizer(x['text']),\n",
        "        batched = True\n",
        "    )\n",
        "    print('Saving dataset to disk...')\n",
        "    tweets_ds.save_to_disk(full_dataset_path)\n",
        "    print(f'Dataset saved at {full_dataset_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "62iMe0NfssL9"
      },
      "outputs": [],
      "source": [
        "# split your dataset into train and val\n",
        "tweets_ds_split = tweets_ds.train_test_split(test_size=TEST_PCT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Untrained BERT Model"
      ],
      "metadata": {
        "id": "AKkJVXXlLgta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and learning rate schedule\n",
        "# original BERT Model trained for 1,000,000 steps total\n",
        "# first 1% (10,000 steps) were warm-up steps w/ static 1e-4 LR, then linear weight decay\n",
        "\n",
        "TOTAL_NUM_STEPS = 100 # to modify after testing compute time\n",
        "num_warmup_steps = math.floor(TOTAL_NUM_STEPS * 0.01)\n",
        "\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=INITIAL_LEARNING_RATE,\n",
        "    num_train_steps=TOTAL_NUM_STEPS,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    weight_decay_rate=WEIGHT_DECAY,\n",
        ")"
      ],
      "metadata": {
        "id": "jCD1o3MKHZLM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig(\n",
        "    vocab_size = VOCAB_SIZE\n",
        ")\n",
        "bert_model = TFBertForMaskedLM(config = config)\n",
        "bert_model.build()\n",
        "bert_model.compile(optimizer = optimizer)\n",
        "\n",
        "bert_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acWz82Z-7Pc",
        "outputId": "fe6f1a39-d902-497e-8014-5fd97b36f2cf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_masked_lm_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  108891648 \n",
            "                                                                 \n",
            " mlm___cls (TFBertMLMHead)   multiple                  24459834  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109514298 (417.76 MB)\n",
            "Trainable params: 109514298 (417.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset for Training Loop"
      ],
      "metadata": {
        "id": "GPenkUQsLnIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YWIdVEP8ssL9"
      },
      "outputs": [],
      "source": [
        "tweets_ds_tf_train = bert_model.prepare_tf_dataset(\n",
        "    dataset = tweets_ds_split[\"train\"],\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = TRAIN_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "tweets_ds_tf_test = bert_model.prepare_tf_dataset(\n",
        "    dataset = tweets_ds_split[\"test\"],\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = TEST_BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Fya59XssL9",
        "outputId": "1d2636b3-10d3-45cf-c1b0-d76e4e6a4ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7a8660278ca0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7a8660278ca0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "10/10 [==============================] - 115s 2s/step - loss: 9.9411 - val_loss: 9.6158\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 6s 617ms/step - loss: 9.2356 - val_loss: 9.1073\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 7s 726ms/step - loss: 8.7189 - val_loss: 8.1799\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 6s 639ms/step - loss: 8.3145 - val_loss: 8.0666\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 6s 638ms/step - loss: 7.9530 - val_loss: 7.3001\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 7s 668ms/step - loss: 7.8054 - val_loss: 7.8600\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 7s 653ms/step - loss: 7.8288 - val_loss: 7.7479\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 6s 627ms/step - loss: 7.4961 - val_loss: 6.8205\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 7s 743ms/step - loss: 7.5246 - val_loss: 7.4987\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 7s 699ms/step - loss: 7.4793 - val_loss: 7.3807\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7a85fc169c60>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "bert_model.fit(\n",
        "    x = tweets_ds_tf_train,\n",
        "    validation_data = tweets_ds_tf_test,\n",
        "    epochs = 10,\n",
        "    steps_per_epoch = 10,\n",
        "    validation_steps = 1,\n",
        "    verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Word Embeddings from Trained Model"
      ],
      "metadata": {
        "id": "ccMN6OrZJkQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_embedding(tokenizer, embedding_layer, token):\n",
        "    # convenience function to get the embedding of a particular token\n",
        "    token_id = wrapped_tokenizer.convert_tokens_to_ids('dog')\n",
        "    return embedding_layer(tf.constant([[token_id]]))"
      ],
      "metadata": {
        "id": "qu6TifH-MoQq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the embedding layer from our bert model\n",
        "embedding_layer = bert_model.bert.embeddings\n",
        "\n",
        "get_token_embedding(\n",
        "    wrapped_tokenizer,\n",
        "    embedding_layer,\n",
        "    'dog'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uVlzFlSKN4v",
        "outputId": "7629e76d-d2ba-480c-b0a9-075eec34f540"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 768), dtype=float32, numpy=\n",
              "array([[[ 7.88608253e-01,  9.68113363e-01,  1.57739028e-01,\n",
              "         -9.38848078e-01,  2.14821160e-01, -1.34463608e+00,\n",
              "         -2.38287240e-01, -7.72886351e-02,  2.11218882e+00,\n",
              "          1.18822300e+00, -8.77568543e-01, -1.21817708e+00,\n",
              "         -9.78406966e-01,  1.11187971e+00,  1.63094485e+00,\n",
              "          2.72095978e-01, -5.70081413e-01, -1.39192247e+00,\n",
              "          7.87784576e-01, -1.60849595e+00, -9.85559225e-01,\n",
              "         -6.31287754e-01, -1.64200544e+00,  8.29331994e-01,\n",
              "          3.43962401e-01, -3.06470066e-01,  3.86720568e-01,\n",
              "          1.11212635e+00,  1.45007074e+00,  2.59744465e-01,\n",
              "          9.94849861e-01, -9.12237465e-01,  9.45935130e-01,\n",
              "         -3.84635329e-01, -8.45617533e-01, -9.22208190e-01,\n",
              "         -4.82530177e-01,  7.82035351e-01,  9.06472802e-01,\n",
              "          6.43554628e-01,  1.09253883e+00, -3.95650268e-01,\n",
              "         -1.09564853e+00, -9.34504569e-01, -9.19237852e-01,\n",
              "          5.71515143e-01, -1.51330042e+00, -1.58704519e-01,\n",
              "         -1.99164617e+00,  8.26031923e-01,  4.52657975e-02,\n",
              "         -2.84798980e-01, -2.55409718e-01, -8.85681093e-01,\n",
              "         -1.34517860e+00, -6.86875939e-01,  2.07747117e-01,\n",
              "          5.05143225e-01, -4.04519677e-01, -3.34324241e-01,\n",
              "          4.55352277e-01, -2.60113806e-01,  5.66087425e-01,\n",
              "          1.10717118e+00,  1.33552659e+00, -1.20601892e+00,\n",
              "         -9.58899796e-01, -8.70185196e-02,  8.42025876e-02,\n",
              "          9.72847819e-01,  2.07930613e+00, -1.72548756e-01,\n",
              "         -9.35085773e-01,  3.80947381e-01, -1.86373174e+00,\n",
              "          6.91728830e-01, -9.78072286e-01, -8.01553547e-01,\n",
              "          1.34970033e+00, -3.68357003e-01, -2.33787251e+00,\n",
              "          6.07870877e-01,  1.49761140e+00,  2.02924538e+00,\n",
              "         -1.92521930e-01,  2.25024134e-01,  5.30243814e-01,\n",
              "          8.33065629e-01,  8.20687711e-01, -7.50335097e-01,\n",
              "          3.32684308e-01,  3.95851851e-01, -8.48912060e-01,\n",
              "         -1.43522692e+00,  2.26293534e-01, -1.02353120e+00,\n",
              "         -1.84877902e-01, -4.24496502e-01,  3.94284844e-01,\n",
              "          5.02274275e-01,  6.03722453e-01,  2.20270538e+00,\n",
              "          1.27525523e-01, -3.59312594e-01, -1.47732604e+00,\n",
              "          1.20291281e+00, -4.23297882e-01, -1.60181093e+00,\n",
              "          1.80223733e-01, -1.65202230e-01,  3.47556710e-01,\n",
              "         -6.37084544e-01,  3.96144718e-01,  2.84790814e-01,\n",
              "         -8.41321170e-01, -2.64004260e-01, -1.35129642e+00,\n",
              "         -1.00531042e+00,  7.20278502e-01, -3.75218272e-01,\n",
              "          5.15304387e-01, -6.46847010e-01, -2.21105123e+00,\n",
              "          4.33977753e-01,  4.63846356e-01,  1.07132840e+00,\n",
              "          1.73506021e+00,  5.02013981e-01, -1.97115028e+00,\n",
              "          1.08955920e+00, -4.98656929e-02, -4.23151582e-01,\n",
              "          1.24052808e-01, -9.84165788e-01, -1.87597573e+00,\n",
              "         -1.15100825e+00, -3.82082433e-01, -9.68640029e-01,\n",
              "         -4.72860456e-01,  7.01522648e-01,  1.81061375e+00,\n",
              "         -3.43833506e-01, -9.05331671e-01, -1.31501651e+00,\n",
              "         -6.62800848e-01,  1.77258289e+00, -1.29589236e+00,\n",
              "          5.44721007e-01,  1.20993006e+00,  1.15123868e+00,\n",
              "          9.37072217e-01,  4.87329543e-01, -6.13584995e-01,\n",
              "          5.47514200e-01, -2.46335357e-01, -2.35185012e-01,\n",
              "         -8.78425360e-01,  7.23316312e-01, -2.51537323e-01,\n",
              "         -1.85178012e-01,  7.48835444e-01,  7.70403624e-01,\n",
              "         -4.47316207e-02, -1.22269011e+00, -2.08312607e+00,\n",
              "         -5.57573199e-01, -1.53442234e-01,  9.27699387e-01,\n",
              "         -1.11092865e+00, -2.03132272e+00,  1.77382147e+00,\n",
              "          2.75030375e-01, -1.04012644e+00, -7.39604890e-01,\n",
              "         -1.05382371e+00, -1.36469960e+00,  8.77516568e-01,\n",
              "         -3.96534652e-01,  1.47075582e+00, -9.07750010e-01,\n",
              "         -1.41112246e-02,  1.56244576e+00, -5.43432176e-01,\n",
              "          4.82710421e-01, -1.49396467e+00, -6.78178489e-01,\n",
              "          1.24124551e+00, -2.62393206e-01,  6.18786097e-01,\n",
              "          1.36628520e+00, -1.42631888e+00,  9.55807328e-01,\n",
              "          1.07858762e-01, -1.19391787e+00, -1.77327171e-01,\n",
              "          5.54044843e-01,  9.89680648e-01,  3.32030654e-01,\n",
              "          1.60532641e+00, -6.82701290e-01, -7.51137495e-01,\n",
              "          7.41443753e-01, -6.75343633e-01,  2.45866847e+00,\n",
              "          1.70063838e-01,  1.97515160e-01,  6.45858943e-01,\n",
              "          1.14570186e-01,  1.23659158e+00, -1.75214994e+00,\n",
              "          1.27722371e+00,  7.18415260e-01, -4.30799305e-01,\n",
              "         -5.15221417e-01, -5.85374415e-01,  1.70108438e+00,\n",
              "         -8.62528920e-01,  1.65211165e+00, -3.94294947e-01,\n",
              "          9.88390088e-01, -9.90599692e-01, -1.42736703e-01,\n",
              "         -2.07182193e+00, -2.68331218e+00,  8.46106648e-01,\n",
              "         -2.26886249e+00,  2.21724510e-01, -6.30307615e-01,\n",
              "         -3.82108301e-01, -1.69458270e+00, -3.83401327e-02,\n",
              "          1.06122172e+00, -1.36875141e+00, -1.85234571e+00,\n",
              "         -2.42368388e+00,  3.19049537e-01,  3.04989994e-01,\n",
              "          7.13897586e-01,  1.53740382e+00, -7.69943520e-02,\n",
              "         -4.12825078e-01,  5.47107339e-01,  1.07241070e+00,\n",
              "         -1.28644347e+00,  1.52874279e+00,  6.82947040e-03,\n",
              "         -7.41540611e-01,  4.64210093e-01,  1.40611029e+00,\n",
              "         -1.64676332e+00,  6.83579206e-01,  8.17398965e-01,\n",
              "          1.30129802e+00,  1.43784571e+00, -8.06634963e-01,\n",
              "         -2.52353698e-01, -1.17062223e+00, -1.27949941e+00,\n",
              "          1.88280165e+00, -1.08334875e+00,  1.61134064e-01,\n",
              "         -3.98760170e-01,  1.42567122e+00,  2.26343602e-01,\n",
              "          3.17216754e-01,  4.56577361e-01, -1.98215365e-01,\n",
              "          2.04198170e+00, -5.04632235e-01,  3.89117002e-01,\n",
              "          7.13130236e-01, -1.60100877e-01,  1.41169220e-01,\n",
              "          2.70857334e-01, -2.61303782e-01, -4.40708578e-01,\n",
              "         -3.93292844e-01,  7.76721776e-01, -4.79975253e-01,\n",
              "         -9.65137541e-01, -1.55910814e+00, -4.03553188e-01,\n",
              "          4.92006481e-01, -1.15032697e+00, -8.45780969e-01,\n",
              "          3.83219011e-02,  1.81649029e+00,  4.12716180e-01,\n",
              "          9.48164225e-01, -3.28197449e-01, -7.77688801e-01,\n",
              "          1.16125762e+00,  7.76459634e-01,  1.24303436e+00,\n",
              "          7.75675535e-01, -1.93674350e+00,  1.05064714e+00,\n",
              "         -1.80989057e-02,  3.80816996e-01, -1.41233206e+00,\n",
              "         -1.28400016e+00, -8.09616506e-01,  9.95250821e-01,\n",
              "          7.54621983e-01,  2.31790274e-01,  1.43831933e+00,\n",
              "          3.84339899e-01,  7.62131691e-01, -1.13768160e+00,\n",
              "          1.32708192e+00,  4.72281098e-01,  1.00872719e+00,\n",
              "         -8.75013709e-01,  1.02878070e+00,  2.79593855e-01,\n",
              "         -4.95158613e-01, -2.52175736e+00, -4.73593920e-01,\n",
              "         -2.61759233e+00,  3.64187896e-01, -2.69718140e-01,\n",
              "         -5.98519802e-01, -7.11822063e-02, -2.16406751e+00,\n",
              "         -1.75715193e-01,  1.08852610e-01, -2.10940316e-01,\n",
              "         -5.21267891e-01,  7.35276818e-01,  1.36520958e+00,\n",
              "          1.16664231e-01,  1.02079988e+00, -1.15097880e+00,\n",
              "         -6.29398078e-02,  9.74686980e-01,  1.05911541e+00,\n",
              "          2.03709650e+00, -2.50472337e-01,  9.46134090e-01,\n",
              "         -3.29013139e-01, -1.07468867e+00,  9.84236747e-02,\n",
              "         -4.18625146e-01,  1.98118305e+00,  1.77308723e-01,\n",
              "          9.48979110e-02,  2.09392953e+00,  1.11152792e+00,\n",
              "          2.02839065e+00, -5.66189408e-01,  1.55760634e+00,\n",
              "          5.50833903e-02,  1.05579662e+00, -8.14253151e-01,\n",
              "          1.32192278e+00, -9.84839499e-01,  9.31802988e-01,\n",
              "         -3.68907630e-01, -1.84565222e+00,  8.45037460e-01,\n",
              "         -1.03713024e+00, -3.95506740e-01,  2.32573092e-01,\n",
              "          6.56305969e-01,  1.35477886e-01,  8.86227131e-01,\n",
              "          1.18417597e+00,  3.04751605e-01, -1.71338332e+00,\n",
              "          8.26173842e-01, -9.36262310e-04, -1.00609660e-01,\n",
              "         -1.14855635e+00, -1.73844421e+00,  1.74243522e+00,\n",
              "         -1.11419928e+00,  7.32320249e-02,  7.99082965e-02,\n",
              "         -9.02046382e-01, -1.52503610e-01,  5.62512696e-01,\n",
              "          2.91134715e-01, -4.41125393e-01, -1.89715958e+00,\n",
              "          7.95031607e-01, -4.43500191e-01,  6.12643883e-02,\n",
              "         -1.40576804e+00, -9.86659527e-01,  4.66973260e-02,\n",
              "         -6.53573573e-01,  1.27450585e+00,  9.83728826e-01,\n",
              "         -1.01431668e+00,  2.63357663e+00, -2.23693848e+00,\n",
              "          6.48885146e-02,  4.22385871e-01,  5.43706536e-01,\n",
              "          4.84799594e-01, -6.23156950e-02, -3.31915259e-01,\n",
              "          1.46323538e+00,  1.12989151e+00, -3.85216951e-01,\n",
              "          9.29712594e-01,  4.25472111e-01, -3.18543673e-01,\n",
              "          3.16898942e-01, -6.70587182e-01,  1.21071748e-01,\n",
              "          1.82584450e-01, -6.27483785e-01, -6.64352655e-01,\n",
              "         -8.86019692e-03, -1.57712936e+00,  1.42518938e+00,\n",
              "          7.35786796e-01,  5.47291994e-01, -1.71387291e+00,\n",
              "          1.73888639e-01, -1.48173749e+00, -2.47618937e+00,\n",
              "         -1.02276552e+00, -9.58076715e-01,  1.58446884e+00,\n",
              "          1.06604064e+00,  8.74373853e-01, -1.07220434e-01,\n",
              "          7.56055534e-01,  1.38401496e+00, -2.91484654e-01,\n",
              "          3.20571274e-01,  3.71899188e-01,  9.42851245e-01,\n",
              "         -1.03402324e-01, -1.24096179e+00,  3.22746009e-01,\n",
              "          8.75501871e-01, -5.33942759e-01, -7.64764845e-05,\n",
              "          6.85407519e-02, -9.04461861e-01,  2.89214283e-01,\n",
              "         -1.09848619e+00,  1.20478797e+00,  1.11896038e+00,\n",
              "         -7.99602389e-01, -1.39415145e-01, -3.33857447e-01,\n",
              "         -6.26123130e-01,  1.10083115e+00,  6.31504655e-01,\n",
              "         -6.07597888e-01,  1.03739309e+00,  1.14369206e-02,\n",
              "          4.67161089e-01,  1.34241939e-01, -1.81374326e-01,\n",
              "         -1.11628449e+00,  2.42962800e-02,  1.71218681e+00,\n",
              "          2.29450628e-01, -1.84202659e+00, -9.49913979e-01,\n",
              "         -9.75664377e-01,  1.63638639e+00,  1.50344551e+00,\n",
              "          2.32036814e-01,  3.47974896e-01, -1.59724069e+00,\n",
              "         -1.42025733e+00,  1.10635293e+00,  1.34859607e-02,\n",
              "         -7.15342999e-01, -1.15506977e-01, -5.81623077e-01,\n",
              "         -1.81271172e+00, -5.95843017e-01, -9.21373069e-01,\n",
              "          1.36686790e+00, -1.19988596e+00, -2.16398716e+00,\n",
              "          7.07991183e-01, -2.53172219e-01,  2.48586655e+00,\n",
              "          7.16158867e-01, -7.01342523e-01,  1.23531032e+00,\n",
              "          1.55583680e+00,  1.83422670e-01, -1.13082862e+00,\n",
              "         -2.43813783e-01,  2.13665336e-01, -1.59923089e+00,\n",
              "         -5.57711840e-01,  1.10519993e+00, -5.90560615e-01,\n",
              "          5.53377509e-01,  6.66231751e-01, -2.32318282e-01,\n",
              "         -1.10746443e+00, -1.47093308e+00,  1.96710110e-01,\n",
              "         -1.58850479e+00, -1.28375340e+00,  3.01736206e-01,\n",
              "         -1.00486845e-01, -8.94983530e-01,  2.44906449e+00,\n",
              "          3.03637952e-01,  2.81419307e-01,  8.76100123e-01,\n",
              "          6.06894672e-01, -1.89829218e+00,  1.71884739e+00,\n",
              "          1.87308407e+00,  6.13987073e-02, -4.79292125e-01,\n",
              "          5.70395470e-01,  2.24404752e-01,  8.88284087e-01,\n",
              "          3.19261849e-01, -1.63690895e-02,  2.55906098e-02,\n",
              "          8.74774992e-01,  8.22791159e-02,  1.53233241e-02,\n",
              "          9.91949081e-01, -1.34396970e+00, -9.49838310e-02,\n",
              "          5.51518798e-01,  6.63816929e-01,  2.17336440e+00,\n",
              "         -9.22221065e-01,  8.87173414e-01,  5.04491292e-02,\n",
              "         -4.04384524e-01,  2.18775421e-01,  5.79982810e-02,\n",
              "          3.20241958e-01, -2.08967820e-01, -2.23093319e+00,\n",
              "         -6.12044930e-01, -4.96948153e-01,  6.30643129e-01,\n",
              "         -1.44182420e+00, -1.37383133e-01,  5.20916760e-01,\n",
              "         -1.45548093e+00,  1.82882762e+00, -7.55358636e-01,\n",
              "          3.08674097e-01, -7.53110111e-01, -1.95306361e-01,\n",
              "         -4.73008394e-01,  6.68918908e-01, -2.97967583e-01,\n",
              "          3.86794835e-01,  1.51762962e+00,  2.06432164e-01,\n",
              "          2.65671194e-01,  1.03722703e+00, -9.56661627e-02,\n",
              "         -1.61278892e+00,  1.66187495e-01,  5.02209961e-01,\n",
              "          9.77803409e-01,  1.57625961e+00, -1.98582804e+00,\n",
              "          2.68020362e-01,  6.94164753e-01, -7.14719817e-02,\n",
              "         -6.16554856e-01, -1.61821663e+00,  8.68336320e-01,\n",
              "          3.66837978e-01,  1.65330863e+00, -6.57754540e-01,\n",
              "         -8.30278933e-01,  1.81647778e-01, -2.06673428e-01,\n",
              "         -6.89135849e-01,  1.07001700e-02, -2.86277890e-01,\n",
              "          9.80563998e-01, -5.91827869e-01,  1.45142221e+00,\n",
              "         -5.93309402e-01, -1.71687591e+00, -1.57574415e+00,\n",
              "         -6.59020424e-01, -7.32485414e-01, -6.55642152e-01,\n",
              "          5.60879469e-01, -2.23330069e+00,  1.37651145e+00,\n",
              "         -8.03615078e-02,  3.05200517e-01,  2.11253762e+00,\n",
              "         -4.07985210e-01,  1.69238472e+00, -6.30358234e-03,\n",
              "          7.49042630e-01,  1.38967186e-01,  6.51968122e-01,\n",
              "         -6.46922886e-01,  1.90662050e+00, -2.34691560e-01,\n",
              "         -2.08318397e-01,  1.28179908e+00, -6.55419409e-01,\n",
              "         -7.48545051e-01,  2.31550232e-01,  1.29277617e-01,\n",
              "          1.16476703e+00, -4.95988905e-01, -7.01114714e-01,\n",
              "         -4.25377488e-01,  7.50987232e-01, -1.12951398e+00,\n",
              "         -7.05595374e-01,  8.99314404e-01, -7.29183435e-01,\n",
              "          6.72900558e-01, -3.72255504e-01,  1.09441781e+00,\n",
              "          9.93889943e-03, -1.35390294e+00, -6.16491027e-02,\n",
              "          2.90569007e-01,  1.17960525e+00, -4.01610881e-01,\n",
              "         -1.18916297e+00,  1.56988096e+00,  1.11144841e+00,\n",
              "          2.87300795e-01,  5.17043412e-01,  2.50811160e-01,\n",
              "          1.37294412e-01, -5.87560713e-01, -5.47495604e-01,\n",
              "         -1.45756125e+00, -4.64214236e-01, -3.60615641e-01,\n",
              "         -1.45146400e-02, -7.67557025e-01, -8.06826174e-01,\n",
              "         -1.60619378e-01,  2.26925626e-01, -1.38415563e+00,\n",
              "          1.04178309e+00,  2.25968146e+00,  6.42862260e-01,\n",
              "          1.68249393e+00, -8.97447765e-01,  6.58742607e-01,\n",
              "         -1.47501218e+00,  2.79162765e-01, -1.67240703e+00,\n",
              "         -1.08907783e+00, -2.24271107e+00, -1.80860683e-01,\n",
              "          2.33465815e+00, -1.94801018e-01, -1.10487068e+00,\n",
              "          5.38324788e-02, -1.50184929e+00, -2.08292231e-02,\n",
              "          8.75970840e-01, -5.92628002e-01,  6.58632517e-01,\n",
              "          1.66742337e+00, -5.63760638e-01,  7.24342465e-01,\n",
              "         -1.05242991e+00,  7.82627285e-01,  1.36867964e+00,\n",
              "         -5.02307951e-01, -8.52011263e-01,  2.09597751e-01,\n",
              "         -6.82245865e-02,  2.58243620e-01, -3.45577329e-01,\n",
              "          3.07010502e-01, -1.69760481e-01, -9.51169431e-01,\n",
              "          2.42655545e-01, -2.27487639e-01,  1.05543971e+00,\n",
              "         -1.27626717e-01,  8.88553679e-01, -1.22981501e+00,\n",
              "         -1.48877025e+00, -1.01105833e+00,  3.19037259e-01,\n",
              "         -7.32418120e-01,  1.12857914e+00, -1.48654342e+00,\n",
              "         -5.31204045e-01, -9.73372698e-01, -4.14651036e-01,\n",
              "          4.12864722e-02,  4.54213977e-01,  1.45432305e+00,\n",
              "          4.73152399e-01,  7.17640221e-01,  9.84035373e-01,\n",
              "          1.51125419e+00,  1.09269643e+00, -1.18625808e+00,\n",
              "         -1.15276730e+00,  1.40272021e+00, -2.34651470e+00,\n",
              "          1.38563859e+00,  4.82651979e-01,  1.77547884e+00,\n",
              "         -1.47282630e-02,  1.22028399e+00,  7.08664417e-01,\n",
              "         -1.07960260e+00,  1.72205210e-01,  1.24858301e-02,\n",
              "         -1.42290801e-01,  1.61924347e-01, -9.97276455e-02,\n",
              "         -1.51231873e+00, -8.15424383e-01,  2.08098460e-02,\n",
              "          8.14954162e-01, -1.23256266e+00, -3.71413648e-01,\n",
              "          5.99350706e-02,  7.69445837e-01,  4.92726803e-01,\n",
              "         -8.76605868e-01,  2.36714125e+00, -1.02310741e+00,\n",
              "         -5.83019219e-02, -9.67710197e-01, -6.23396456e-01,\n",
              "          1.69633234e+00, -8.55884671e-01,  5.51875681e-04,\n",
              "          1.19172919e+00, -1.28542781e+00,  3.18548828e-01,\n",
              "          7.79092729e-01, -1.13976228e+00,  1.40713573e+00,\n",
              "          7.00481772e-01, -3.05211723e-01,  2.67335713e-01,\n",
              "         -1.67742252e-01,  4.15040851e-01, -8.75598341e-02,\n",
              "         -3.20173055e-01, -8.11029077e-02, -4.23802048e-01,\n",
              "         -8.94636363e-02, -6.82129681e-01,  9.58838224e-01,\n",
              "          4.43209499e-01,  2.77352512e-01,  1.57983482e-01,\n",
              "         -7.61025190e-01, -3.13485295e-01,  2.18868470e+00]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9QYyNRXnLTCE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.distilbert.embeddings.word_embeddings([\"my token ids here\"])"
      ],
      "metadata": {
        "id": "H4WuI5Pc2vDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}