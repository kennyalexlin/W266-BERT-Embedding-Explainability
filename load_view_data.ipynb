{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request, json \n",
    "\n",
    "from transformers import (\n",
    "    BertConfig, \n",
    "    TFBertForMaskedLM\n",
    ")\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    processors,\n",
    "    decoders\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    get_tweet_list,\n",
    "    get_tweet_iterator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "TWEETS_PATH = '../data/birthyear.1950_1969.lowercase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Untrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  86218752  \n",
      "                                                                 \n",
      " mlm___cls (TFBertMLMHead)   multiple                  1757416   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86811880 (331.16 MB)\n",
      "Trainable params: 86811880 (331.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# not needed - BertConfig default settings are equivalent to bert-base-uncased config\n",
    "# with urllib.request.urlopen(\"https://huggingface.co/google-bert/bert-base-uncased/resolve/main/config.json\") as url:\n",
    "#     config_from_pretrained = json.load(url) # bert-base-uncased config\n",
    "    \n",
    "# initialize & build Masked LM BERT model w/ default config settings\n",
    "config = BertConfig(\n",
    "    vocab_size = VOCAB_SIZE\n",
    ")\n",
    "bert_model = TFBertForMaskedLM(config = config)\n",
    "bert_model.build()\n",
    "\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train BERT Tokenizer\n",
    "https://huggingface.co/learn/nlp-course/en/chapter6/8#building-a-wordpiece-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BERT tokenizer\n",
    "tokenizer = Tokenizer(model = models.WordPiece(unk_token = '[UNK]'))\n",
    "normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),            # Normalize characters to Unicode NFD\n",
    "    # normalizers.Lowercase(),      # Set all characters to lowercase - not necessary, as tweets are already lowercase\n",
    "    normalizers.StripAccents()    # Remove all accents from characters\n",
    "])\n",
    "pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# create a WordPiece trainer\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    special_tokens = special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator out of our tweets\n",
    "tweets = get_tweet_iterator('../data/birthyear.1950_1969.lowercase')\n",
    "tokenizer.train_from_iterator(tweets, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = get_tweet_list(TWEETS_PATH)\n",
    "tokenizer.train_from_iterator(tweets, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", tokenizer.token_to_id('[CLS]')), (\"[SEP]\", tokenizer.token_to_id('[SEP]'))],\n",
    ")\n",
    "tokenizer.post_processor = post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.decoder = decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
